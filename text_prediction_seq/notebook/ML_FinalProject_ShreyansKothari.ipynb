{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vkw43rsHr6V"
      },
      "source": [
        "# *Text Prediction using Sequential Models*\n",
        " **UNI:** sk4819 | **Name:** Shreyans Kothari\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvwKUcDkJD64"
      },
      "source": [
        "### 1) Problem: \n",
        "Text prediction models utilize Natural Language Processing and Machine Learning to predict the subsequent word(s) in a sentence. The user inputs a few words and the model predicts the words that are most likely to come after them. Text predicition models have a lot of use, depending on the industry and field they are employed in. They are quite ubiquitious and assist us on a daily basis, sometimes without us even realizing. If you go to Google and type in a few words in the search bar, Google's autofill feature completes the sentence for you without making you type all the words- saving your precious time. On gmail, the model learns your writing style from the emails that you send and over time starts reccomending words/phrases. Instead of you having to type \"Dear Prof. Morales, I hope you're doing well!\", all you have to do is start typing \"Dear..\" and gmail does the rest for you- again saving a lot of time.\n",
        "\n",
        "\n",
        "The biggest problem models like these solve is that of speed. Text prediciton models increase efficiency in different processes, thus increasing the speed with which pepole conduct business, talk to each other, and live their lives. As these models keep getting faster and more accurate, the ease in our interactions and communication keeps improving. In a sense, text prediction models increase the speed of development of our economies by allowing people to find the right information and connect with the right people in an appropriate and efficient manner.\n",
        "\n",
        "\n",
        "### 2) Data: \n",
        "I wanted to takle this problem in a slightly different way. I wanted to create a model that would be personalized to me and my writing style.\n",
        "\n",
        "I trained the model on papers, emails, and creative writings (short stories, poems, incomplete long-er stories, etc.) that I have written before and at graduate school. I thought it would be interesting to teach the model my style. I put all the texts in one single document and imported it to Python using the textract package.\n",
        "\n",
        "In addition to my personal texts, I also decided to include a text corpus from the nltk library: the WSJ articles corpus in order to increase the number of unique tokens and expand the scope of the model. I didn't do any preprocessing other than combining all texts, removing blank spaces and punctuation, and converting all words to lowercase. The final dataset had a total of 168,719 words, out of which 13,673 were unique. \n",
        "\n",
        "\n",
        "### 3) Deep Learning:\n",
        "In order to build a text prediciton model, we need to use a sequential model because the sequence of the words matter. If we tokenized all words and just treated them as separate points, we would not be able to get a model that would be able to predict the subsequent words. The Sequential Deep Learning models that I use in this project allow the model to actually learn the \"right\" order of words to output when given any words. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_F5ql-tJJid"
      },
      "source": [
        "### Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2A0q_0h4_7U"
      },
      "outputs": [],
      "source": [
        "!pip install textract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9VBTC1hJIyh",
        "outputId": "6272174b-48ef-4ef5-837f-ded6342e1a00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "#import docx\n",
        "import textract\n",
        "from google.colab import files\n",
        "import pickle\n",
        "path_in = \"/content/drive/MyDrive/ML_FinalProject/\"\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsErhJrA5iwU",
        "outputId": "3e3e5dfe-761d-4cc7-bd06-6b04e7d5ff39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ML_FinalProject\n"
          ]
        }
      ],
      "source": [
        "cd \"/content/drive/MyDrive/ML_FinalProject/\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Data"
      ],
      "metadata": {
        "id": "H5jWvGsDeKG8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IUHFY-eI7V56"
      },
      "outputs": [],
      "source": [
        "text1 = textract.process(\"/content/drive/MyDrive/ML_FinalProject/train1.docx\")\n",
        "text2 = textract.process(\"/content/drive/MyDrive/ML_FinalProject/train2.docx\")\n",
        "text3 = textract.process(\"/content/drive/MyDrive/ML_FinalProject/train3.docx\")\n",
        "text1 = text1.decode(\"utf-8\") \n",
        "text2 = text2.decode(\"utf-8\") \n",
        "text3 = text3.decode(\"utf-8\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RpFkoy3Cq39"
      },
      "outputs": [],
      "source": [
        "nltk.download(\"book\")\n",
        "from nltk.book import text7\n",
        "text7 = str(' '.join(text7[:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "U7YyR0EFTywX"
      },
      "outputs": [],
      "source": [
        "text =  text1  + \" \" + text2 + \" \" + text3 + \" \" + text7"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Text Excerpts"
      ],
      "metadata": {
        "id": "0m13JBDOelWN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "G0_rsTbMT3P5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "outputId": "c6227e93-7e41-4f35-b323-8372d8af26c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ng their collective bargaining power will leave the management with no choice but to invest in improving the workers’ living conditions. A union led by the workers will also validate the negotiation for better healthcare, daily quotas, and wage outcomes for both permanent and temporary workers. Restricted Flow of Information The Assamese tea farmers in large plantations are greatly reliant on the Tocklai Tea Research Association, a research institute established by the Tea Board of India, for information on fertilizer and pesticide use, water irrigation management, soil erosion, and other sustainable farming practices. However, the smallholder farmers have been unable to benefit from these land management and farming best practices due to accessibility issues. The information is made available in systems and formats that lie outside the capacity and reach of small plantation owners. The lack of information affects not only the smallholder farmers’ productivity and yields (and consequen'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "text[10000:11000]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text[20000:21000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "CiFcswVkep-_",
        "outputId": "b7890e82-a94f-40b8-b394-e94d13f24dde"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'s. KalaaCo is extremely thankful to your team for all your help in facilitating the translation of KalaaCo from a simple idea into a tangible social enterprise that seeks to improve the market outcomes for small-scale artisans in Rajasthan, India.\\n\\nPlease feel free to reach out to us at sk4819@columbia.edu if you have any follow up questions or comments. The Rajasthani artform encompasses traditions and techniques that have been passed down generations, and customs that have been practiced for over decades (some over a millennia). An untrained eye might fail to notice the influences of the various empires – Mauryan, Rajput, Mughal, Hindu, British, etc. – that ruled over this region. The hundreds of thousands Rajasthani craftspeople, artisans, and handicraft workers constantly traverse the intersection of a myriad cultural and ethnic identities. The intricacies and the delicate-nature of their art renders these small-scale artists unable to compete with large-scale corporations whose ma'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text[100000:110000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "G5T60Ojeesj4",
        "outputId": "2cdcb202-cfa1-4a7f-d564-2e2cb9e19be1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ountless times, “Stop! Open your eyes, you dimwits! Look at how senseless all this walking is!” Whenever this feeling starts overpowering me, I have to calm myself down by looking away. Statues are not supposed to have thoughts like these, you know. Don’t let me down He told me plainly, \"It is up to you now. Don\\'t let me down.\"\\xa0 It was a warm evening and Raul was playing with his toy car in the backyard. I had just put a teapot on the stove when my father walked in.\\xa0 \"Would you like some tea?\"\\xa0 \"Why, yes. Do you have any of the Verbena Lemon left? I\\'ll have some of that. Please and thank you.\" I took out the blue ceramic cups one aunt Galdys had gifted me on my wedding day. The blue had started to dull over the years but I could never bring myself to throw them away. They were Peter\\'s favorite cups and throwing them away felt like I was betraying him. I poured the hot brown liquid into the cups and sat down across from my father. He was lost in his thoughts and I let him be. Over the past few weeks since I moved in, we had gotten into the habit of speaking only when it was necessary. We enjoyed each other\\'s company- the silence didn\\'t bother us. I took in a big gulp and put down my cup on the table gently. From the corner of my eyes, I saw his earnest smile staring back at me from an old photograph on the wall. I bit my lips hard to hold back my tears. My father noticed and his face fell. He let out a huge sigh and abruptly walked out the back door into the yard. I saw his reflection in the mirror hanging above the counter, Papa picked Raul up on his shoulders as the creaky door slowly swung shut. Deep shades of blue and red covered the cloudy sky. The weather had started to cool down and a few gusts escaped in through the crack under the door. I let out a sigh and ran my left hand through my hair. I wait here again, it’s all I do. Day in day out, I wait for you.\\xa0 When you don’t come home, Late at night, I close my eyes, To dream of you. Your hand on my back, The quirks that I hate, Your happy dance, I think of you.\\xa0 Day in day out, I wait for you, It’s all I do. Dare I call us friends? After the four drinks we shared, On two separate occasions? If you smile every time, I run into\\xa0 Dare I call us close? If we shared our dreams, And our worst nightmares? Let me go,” he told me plainly. A few years ago, we fell into the habit of speaking only when necessary, as if to prepare for what was to come. We stopped kissing goodbye before walking out the door and we failed to embrace each other under the sheets at night. No more did we rest our eyes while talking after a long day, or call around noon just to hear each other’s voice. The humdrum of our lives filled up the silence in our home as we lost touch living under the same roof.\\xa0 These days, I wake up to the hissing sound of his ventilator. I try to keep aware of its rhythm as I go about my day, when it pushes air in and when it reinflates- but I often find myself unknowingly tuning it out. The rare doorbell does anything but deliver a respite from the new montony. Every morning, I mindlessly wipe his entire body with a white cloth dipped in lukewarm water; not even a hint of a once plump figure on his protruding ribs and his sunken cheeks. I empty his bedpan and clean his side table that is lined up with little orange bottles with white caps. He lays there with his eyes wide open, staring at me the entire while. His infrequent blinks remind me that he’s still present. I did not realize when I stopped loving him. I did not even know that I didn’t until I found out he had less than a month left and all I could feel was the utmost indifference. The night before he passed, he came to me in a dream. He was wearing the dark blue polo that I had bought on a whim. He stood there silently, watching me watch him. Every breath he took felt longer than the last. The unexpected longing I felt for what we used to be was replaced by dread when he finally spoke to me. “Let me go,” he said in the hushest of tones. The next morning I awoke again with the hissing of the ventilator. When I tried to clean him with the same white cloth, I realized he never opened his eyes. Just past the animated crowds, Loud, oblivious, and pitiless. The despotic sirens and superfluous honks, Drowning out their unending howls.\\xa0 Past the unaware tot tugging at her leash, Tethered to the stroller with a swanky sunroof, From which rules a haughty pup-Its snout raised high. Beyond the stocky men, Huddled around their bikes, Cladding bags shaped like boxes, Just about plumper than their paunches.\\xa0 After the nonsensical woman, Ungiving of the raucous, Swaying jauntily to every beat, Flowing willfully from her tattered device. Right across the brimming brownstone, Home to a brood of discolored mice- All safe within their squatter rights.\\xa0 With creaking floors and clunky heaters that screech, In praise of the fickle-minded weather. Along the table of swarthy men, Consumed in an uninhibited match, Hatching plans with the utmost attention To set down each fading monochrome tile.\\xa0 Inside the building-dressed in scaffolding, As old as the bricks it embodies. Through the scratched Red Maple door, Just up the trembling stairs, Neglected for a little too long.\\xa0 My eyes were heavy from sleep but I sat right up. The glass window on my front was dusty and there was a hole in the mosquito screen. A few unidentifiable bugs had flown in through the hole and were now trapped in between the closed window and the mesh screen. The small black fan on the floor on the right side of the bed making a whirring sound was acting as a white-noise machine, filling the dull room with a sleepy atmosphere. The air-mattress I was using as my bed had deflated a lot over the night. I was now sitting in a cavity in the shape of my body.\\xa0 The box-shaped room had no furniture except that air-mattress, if that even counted as furniture, and a bed-side table fashioned from an old cardboard box with a navy blue cotton table-cloth neatly draped over it. A half empty pack of cigarettes and a sky-blue asthma inhaler were lying on top of it. There was something sardonic about how they touched each other. Two star-crossed lovers. An odd-looking-couple with clashing personalities. But love works in mysterious ways.\\xa0\\n\\nI rubbed my eyes and yawned twice before I finally found the strength to get out of my deflated bed.\\xa0 I lit up a cigarette after turning on the coffee machine. The menthol fragrance of the cigarettes intermingled with the strong aroma of Ethiopian coffee creating a distinct smell that the apartment had been collecting every day sincerely since I moved in two weeks ago.\\xa0 With freshly brewed black coffee in a blue cup that my parents gifted me along with a set of plates for my house-warming, in my right hand and a cigarette held between the index and middle finger of my left hand I walked out to the mailbox.\\xa0 Walking out of the apartment, I looked down to just glance at my bare chest and the dark grey pyjamas that I had owned for the past four years. They used to be jet black when I first bought them, but continuous wearing and washing had faded the colour. The small hole that had found its home in the right pocket a couple of months ago and had grown tremendously in size. A gluttonous man who got way too many free meals.\\xa0 Holding the cigarette between my inverted lips, I transferred the cup of coffee to my left hand and opened the mailbox with my now free right hand. A flyer for a new restaurant down the street. A 50% off coupon for ordering groceries online. My internet bill. A letter from my insurance company. And a long white envelope with my name scribbled neatly in one corner and an Indian flag stamp on the other corner. The very first memory I have of him is from when I was seven years old. Although if I try hard enough I am sure I can remember some older memories of him, but that would take me a while and would not result in a coherent story, rather, it would just be a bland explanation of the ‘image’, much like the description of a painting in a museum. Near our haveli in the old city of Jaipur, there is an all-girls school which was started by the beloved Maharani Gayatri Devi to increase female education. It is a public school, and thus, follows all the public projects and programs the government ‘advises’ them to. One of such programs was a free polio camp for children from around its neighborhood. Aren’t we all just stories in the end? Talked about over a coffee, or a drink. Over a warm dinner on a humid summer evening. We become words of aggression that fight the silence. A meek silence we thought we didn’t want. Until all that is left is the fleeting feeling of longing, which it carried in its womb. The longing for the peace the silence birthed. The one we forced away, just because this silence was ‘awkward’, as we call it.\\xa0 Aren’t we all just memories of a future self? Thought about on an unmade bed under a blanket too hot. Or on a messy desk we promised to clean yesterday.\\xa0 Memories are just personal stories we narrate everyday. Experiences we told ourselves would teach us what’s right. Hi Greg, I hope your weekend is going well! Apologies for sending you an email on a Sunday. I am currently working on Lab3, wherein I am using a dataset that contains CO2 emissions for all US states between the years 1980 and 2018. I chose to do option #4, the FE model for this lab. I am a little unsure about what I should be indexing the plm model on since I have two fixed variables. I did a little research on Google and came across a two-way fixed effects model but that seems a little difficult to interpret. My understanding is that I would look at the state as the fixed variable since I am analyzing the effects of state-level minimum wage and income inequality on state-level carbon emissions. However, the temporal aspect is important for the model too- do you have any suggestions on how I can incorporate both without complicating things to'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing"
      ],
      "metadata": {
        "id": "-qauJ-v4e56O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1g2OzArdjKx",
        "outputId": "d097e412-651d-4e78-e5da-b19d54fd8638"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Tokens: 168719\n",
            "Unique Tokens: 13673\n",
            "These are the first 50 tokens: ['the', 'modern', 'connotations', 'of', 'tea', 'extend', 'beyond', 'the', 'idea', 'of', 'a', 'beverage', 'it', 'is', 'increasingly', 'becoming', 'a', 'symbol', 'of', 'healthy', 'living', 'and', 'well', 'being', 'in', 'addition', 'to', 'its', 'associated', 'health', 'benefits', 'a', 'rapidly', 'growing', 'middle', 'class', 'in', 'emerging', 'countries', 'has', 'boosted', 'global', 'consumption', 'of', 'tea', 'which', 'currently', 'sits', 'at', '5']\n",
            "Total Sequences: 168711\n",
            "This is the first sequence: the modern connotations of tea extend beyond the\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "\n",
        "def clean_doc(doc):\n",
        "    # replace '--' with a space ' '\n",
        "    #doc = doc.replace('--', ' ')\n",
        "    # split into tokens by white space\n",
        "    tokens = doc.split()\n",
        "    # remove punctuation from each token\n",
        "    tokens = [' ' if w in string.punctuation else w for w in tokens]\n",
        "    # remove remaining tokens that are not alphabetic\n",
        "    #tokens = [word for word in tokens if word.isalpha()]\n",
        "    # make lower case\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "    #tokens = tokens.strip()\n",
        "    return tokens\n",
        "\n",
        "def clean_text(txt_in):\n",
        "    import re\n",
        "    clean = re.sub('[^A-Za-z0-9]+', \" \", txt_in).lower().strip()\n",
        "    clean = clean.split()\n",
        "    return clean\n",
        " \n",
        "tokens = clean_text(text)#clean_doc(text)\n",
        "\n",
        "number_of_unique_tokens = len(set(tokens))\n",
        "\n",
        "print('Total Tokens: %d' % len(tokens))\n",
        "print('Unique Tokens: %d' % number_of_unique_tokens)\n",
        "print('These are the first 50 tokens: %s' % tokens[:50])\n",
        "\n",
        "# A key design decision is how long the input sequences should be. \n",
        "# They need to be long enough to allow the model to learn the context for the words to predict. \n",
        "# This input length will also define the length of seed text used to generate new sequences \n",
        "# when we use the model.\n",
        "# There is no correct answer. With enough time and resources, we could explore the ability of \n",
        "# the model to learn with differently sized input sequences.\n",
        "\n",
        "sequence_length = 7\n",
        "\n",
        "# organize into sequences of tokens of input words plus one output word\n",
        "length = sequence_length + 1 # This was changed to 2 from 1\n",
        "sequences = list()\n",
        "for i in range(length, len(tokens)):\n",
        "    # select sequence of tokens\n",
        "    seq = tokens[i-length:i]\n",
        "    # convert into a line\n",
        "    line = ' '.join(seq)\n",
        "    # store\n",
        "    sequences.append(line)\n",
        "\n",
        "print ('Total Sequences: %d' % len(sequences))\n",
        "print ('This is the first sequence: {0}'.format(sequences[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4) Model Fitting"
      ],
      "metadata": {
        "id": "6BHqGYMke9vo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model 1: Basic LSTM with 200 nodes and 100 epochs"
      ],
      "metadata": {
        "id": "UtIoIj4OhFqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  import numpy as np\n",
        "  from keras.preprocessing.text import Tokenizer\n",
        "  from tensorflow.keras.utils import to_categorical\n",
        "  from keras.models import Sequential\n",
        "  from keras.layers import Dense, Conv1D, Flatten\n",
        "  from keras.layers import LSTM\n",
        "  from keras.layers import Embedding\n",
        "\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(sequences)\n",
        "  sequ = tokenizer.texts_to_sequences(sequences)\n",
        "\n",
        "  vocab_size = number_of_unique_tokens + 1\n",
        "\n",
        "  sequences0 = np.array(sequ)\n",
        "  X, y = sequences0[:,:-1], sequences0[:,-1]\n",
        "  y = to_categorical(y, num_classes=vocab_size)\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocab_size, sequence_length, input_length=sequence_length))\n",
        "  model.add(LSTM(200))\n",
        "  model.add(Dense(100, activation='relu'))\n",
        "  model.add(Dense(vocab_size, activation='softmax'))\n",
        "  \n",
        "  print(model.summary())\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "\n",
        "  model.fit(X, y, batch_size=80, epochs=100)\n"
      ],
      "metadata": {
        "id": "68KHW2myhLuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving model 1\n",
        "pickle.dump(model, open(path_in + \"model1.pkl\", \"wb\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMzL-BZ0hL59",
        "outputId": "f655b226-d2ec-4be7-969e-973f21fc8f33"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: ram://d03a68a4-3be9-4aa5-9402-e01babe44eba/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: ram://d03a68a4-3be9-4aa5-9402-e01babe44eba/assets\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fcf0382e590> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (X.shape)\n",
        "prediction = model.predict(X[0].reshape(1,sequence_length))\n",
        "print (prediction.shape)\n",
        "print (prediction)"
      ],
      "metadata": {
        "id": "Aa0WHe4mgiwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trying different phrases:\n",
        "## \"Raul was playing with his toy car\" --> 'Raul was playing with his toy car in the case whereas supply facilitate china'\n",
        "# \"I think there has been an error\" --> \"I think there has been an error and would tend to a lack of\"\n",
        "# \"I am a dual degree\" --> \"I am a dual degree masters candidate from a several development\"\n",
        "# \"Data Science and Machine Learning \" --> \"Data Science and Machine Learning  practices to address them the artisans\"\n",
        "# \"The artisans from\" --> \"The artisans from the vehicle war and politics programs\"\n",
        "# \"I dont know where to start\" --> \"I dont know where to start more about vox food and economic\"\n",
        "# \"Artificial Intelligence is going to\" --> \"Artificial Intelligence is going to examine the extinction of rajasthani artforms\"\n",
        "test = ['Once upon a time there was a']\n",
        "\n",
        "\n",
        "for t in test:\n",
        "    example = tokenizer.texts_to_sequences([t])\n",
        "    prediction = model.predict(np.array(example))\n",
        "    predicted_word = np.argmax(prediction)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))  # https://stackoverflow.com/a/43927939/246508\n",
        "    t1 = t + \" \" + reverse_word_map[predicted_word]\n",
        "\n",
        "    word2 = tokenizer.texts_to_sequences([t1])\n",
        "    prediction1 = model.predict(np.array(word2))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t2 = t1 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    word2 = tokenizer.texts_to_sequences([t2])\n",
        "    prediction1 = model.predict(np.array(word2))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t3 = t2 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    word2 = tokenizer.texts_to_sequences([t3])\n",
        "    prediction1 = model.predict(np.array(word2))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t4 = t3 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    word3 = tokenizer.texts_to_sequences([t4])\n",
        "    prediction1 = model.predict(np.array(word3))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t5 = t4 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    word4 = tokenizer.texts_to_sequences([t5])\n",
        "    prediction1 = model.predict(np.array(word4))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t6 = t5 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    word5 = tokenizer.texts_to_sequences([t6])\n",
        "    prediction1 = model.predict(np.array(word5))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t7 = t6 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    word6 = tokenizer.texts_to_sequences([t7])\n",
        "    prediction1 = model.predict(np.array(word6))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t8 = t7 + \" \" + reverse_word_map[predicted_word1]  \n",
        "\n",
        "    word7 = tokenizer.texts_to_sequences([t8])\n",
        "    prediction1 = model.predict(np.array(word7))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t9 = t8 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    print(t6)\n",
        "    #print (\"{0} -> {1}\".format(t, reverse_word_map[predicted_word]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1m-IAQs1hL98",
        "outputId": "f60f4406-de79-4287-db54-e6ae3e5db55c"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time there was a touch coated in the western world\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 1 had a validation set score of 0.6066, which is not too bad for the first model. I tried the model to predict a few phrases (above) and most of them make gramatically sense (not contextually tho). It was interesting to see the model give sentences that could somewhat make sense if the context was right. "
      ],
      "metadata": {
        "id": "dLfRqf1ju0uT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model 2: Adding an additional LSTM layer with 100 nodes"
      ],
      "metadata": {
        "id": "ztkVXqFmxV-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  import numpy as np\n",
        "  from keras.preprocessing.text import Tokenizer\n",
        "  from tensorflow.keras.utils import to_categorical\n",
        "  from keras.models import Sequential\n",
        "  from keras.layers import Dense, Conv1D, Flatten\n",
        "  from keras.layers import LSTM\n",
        "  from keras.layers import Embedding\n",
        "\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(sequences)\n",
        "  sequ = tokenizer.texts_to_sequences(sequences)\n",
        "\n",
        "  vocab_size = number_of_unique_tokens + 1\n",
        "\n",
        "  sequences0 = np.array(sequ)\n",
        "  X, y = sequences0[:,:-1], sequences0[:,-1]\n",
        "  y = to_categorical(y, num_classes=vocab_size)\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocab_size, sequence_length, input_length=sequence_length))\n",
        "  model.add(LSTM(200, return_sequences=True ))\n",
        "  model.add(LSTM(100))\n",
        "  model.add(Dense(100, activation='relu'))\n",
        "  model.add(Dense(vocab_size, activation='softmax'))\n",
        "  \n",
        "  print(model.summary())\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "\n",
        "  model.fit(X, y, batch_size=80, epochs=100)"
      ],
      "metadata": {
        "id": "pVwEJhxOhMBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving model 2\n",
        "path_in = \"/content/drive/MyDrive/ML_FinalProject/\"\n",
        "pickle.dump(model, open(path_in + \"model2.pkl\", \"wb\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TV7k-_Cjzb8T",
        "outputId": "fff49215-23e6-44f0-836e-5b2cc2d7e886"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: ram://2d6cde0c-f050-48d5-ba81-be8599f1aec7/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: ram://2d6cde0c-f050-48d5-ba81-be8599f1aec7/assets\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd076d45ed0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd076c8c150> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (X.shape)\n",
        "prediction = model.predict(X[0].reshape(1,sequence_length))\n",
        "print (prediction.shape)\n",
        "print (prediction)"
      ],
      "metadata": {
        "id": "J2QNu8qdgmNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trying different phrases:\n",
        "## Hi how are you doing today\n",
        "# \"Raul was playing with his toy car\" --> 'Raul was playing with his toy car in india and india and india'\n",
        "# \"I think there has been an error\" --> \"I think there has been an error decrease working going to more ways\"\n",
        "# \"I am a dual degree\" --> \"I am a dual degree is i believe you realizes to\"\n",
        "# \"Data Science and Machine Learning\" --> \"Data Science and Machine Learning separate focus on fundamental years peddle\"\n",
        "# \"I dont know where to start\" --> \"I dont know where to start read surrounded out about it harder\"\n",
        "# \"Artificial Intelligence is going to\" --> \"Artificial Intelligence is going to distrust and programs in rajasthan arl\"\n",
        "test = ['Hi how are you doing today']\n",
        "\n",
        "\n",
        "for t in test:\n",
        "    example = tokenizer.texts_to_sequences([t])\n",
        "    prediction = model.predict(np.array(example))\n",
        "    predicted_word = np.argmax(prediction)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))  # https://stackoverflow.com/a/43927939/246508\n",
        "    t1 = t + \" \" + reverse_word_map[predicted_word]\n",
        "\n",
        "    word2 = tokenizer.texts_to_sequences([t1])\n",
        "    prediction1 = model.predict(np.array(word2))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t2 = t1 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    word2 = tokenizer.texts_to_sequences([t2])\n",
        "    prediction1 = model.predict(np.array(word2))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t3 = t2 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    word2 = tokenizer.texts_to_sequences([t3])\n",
        "    prediction1 = model.predict(np.array(word2))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t4 = t3 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    word3 = tokenizer.texts_to_sequences([t4])\n",
        "    prediction1 = model.predict(np.array(word3))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t5 = t4 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    word4 = tokenizer.texts_to_sequences([t5])\n",
        "    prediction1 = model.predict(np.array(word4))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t6 = t5 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    word5 = tokenizer.texts_to_sequences([t6])\n",
        "    prediction1 = model.predict(np.array(word5))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t7 = t6 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    word6 = tokenizer.texts_to_sequences([t7])\n",
        "    prediction1 = model.predict(np.array(word6))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t8 = t7 + \" \" + reverse_word_map[predicted_word1]  \n",
        "\n",
        "    word7 = tokenizer.texts_to_sequences([t8])\n",
        "    prediction1 = model.predict(np.array(word7))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t9 = t8 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    print(t6)\n",
        "    #print (\"{0} -> {1}\".format(t, reverse_word_map[predicted_word]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kg-SEc2gzb_B",
        "outputId": "bfeab873-8f4d-4ee4-ad1c-26cfd5e12067"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi how are you doing today s steps in the past several\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 2's validation set score decreased to 0.4765 from model 1. Seems like adding an additional layer did not improve the model and had the opposite effect. The text predictions the model outputs make even less sense than those from Model 1. Model 1 seems to be better than Model 2."
      ],
      "metadata": {
        "id": "U_EI4xim_qZG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model 3: Model with one SimpleRNN layer and 200 nodes"
      ],
      "metadata": {
        "id": "I-jTLbEVAE6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  import numpy as np\n",
        "  from keras.preprocessing.text import Tokenizer\n",
        "  from tensorflow.keras.utils import to_categorical\n",
        "  from keras.models import Sequential\n",
        "  from keras.layers import Dense, Conv1D, Flatten\n",
        "  from keras.layers import LSTM, SimpleRNN\n",
        "  from keras.layers import Embedding\n",
        "\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(sequences)\n",
        "  sequ = tokenizer.texts_to_sequences(sequences)\n",
        "\n",
        "  vocab_size = number_of_unique_tokens + 1\n",
        "\n",
        "  sequences0 = np.array(sequ)\n",
        "  X, y = sequences0[:,:-1], sequences0[:,-1]\n",
        "  y = to_categorical(y, num_classes=vocab_size)\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocab_size, sequence_length, input_length=sequence_length))\n",
        "  model.add(SimpleRNN(200))\n",
        "  model.add(Dense(100, activation='relu'))\n",
        "  model.add(Dense(vocab_size, activation='softmax'))\n",
        "  \n",
        "  print(model.summary())\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "\n",
        "  model.fit(X, y, batch_size=80, epochs=100)\n",
        "  pickle.dump(model, open(path_in + \"model3.pkl\", \"wb\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBxWRZqQzcCe",
        "outputId": "40e437aa-5e49-480f-cde1-04164336b081"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 7, 7)              95718     \n",
            "                                                                 \n",
            " simple_rnn (SimpleRNN)      (None, 200)               41600     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 100)               20100     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 13674)             1381074   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,538,492\n",
            "Trainable params: 1,538,492\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 7.2053 - accuracy: 0.0602\n",
            "Epoch 2/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 6.8487 - accuracy: 0.0771\n",
            "Epoch 3/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 6.6833 - accuracy: 0.0846\n",
            "Epoch 4/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 6.5735 - accuracy: 0.0922\n",
            "Epoch 5/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 6.4755 - accuracy: 0.1016\n",
            "Epoch 6/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 6.3909 - accuracy: 0.1075\n",
            "Epoch 7/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 6.3126 - accuracy: 0.1118\n",
            "Epoch 8/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 6.2353 - accuracy: 0.1158\n",
            "Epoch 9/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 6.1580 - accuracy: 0.1200\n",
            "Epoch 10/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 6.0891 - accuracy: 0.1242\n",
            "Epoch 11/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 6.0305 - accuracy: 0.1261\n",
            "Epoch 12/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.9750 - accuracy: 0.1287\n",
            "Epoch 13/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.9292 - accuracy: 0.1309\n",
            "Epoch 14/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.8876 - accuracy: 0.1328\n",
            "Epoch 15/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.8499 - accuracy: 0.1344\n",
            "Epoch 16/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.8149 - accuracy: 0.1362\n",
            "Epoch 17/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.7880 - accuracy: 0.1375\n",
            "Epoch 18/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.7547 - accuracy: 0.1387\n",
            "Epoch 19/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.7302 - accuracy: 0.1389\n",
            "Epoch 20/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.7094 - accuracy: 0.1409\n",
            "Epoch 21/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.7004 - accuracy: 0.1414\n",
            "Epoch 22/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.6671 - accuracy: 0.1425\n",
            "Epoch 23/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.6492 - accuracy: 0.1434\n",
            "Epoch 24/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.6329 - accuracy: 0.1441\n",
            "Epoch 25/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.6171 - accuracy: 0.1454\n",
            "Epoch 26/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.6084 - accuracy: 0.1457\n",
            "Epoch 27/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.5885 - accuracy: 0.1470\n",
            "Epoch 28/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.5782 - accuracy: 0.1467\n",
            "Epoch 29/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.5648 - accuracy: 0.1476\n",
            "Epoch 30/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.5538 - accuracy: 0.1479\n",
            "Epoch 31/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.5622 - accuracy: 0.1494\n",
            "Epoch 32/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.5451 - accuracy: 0.1494\n",
            "Epoch 33/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.5289 - accuracy: 0.1500\n",
            "Epoch 34/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.5204 - accuracy: 0.1509\n",
            "Epoch 35/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.5093 - accuracy: 0.1507\n",
            "Epoch 36/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.4987 - accuracy: 0.1520\n",
            "Epoch 37/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.4877 - accuracy: 0.1520\n",
            "Epoch 38/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.4794 - accuracy: 0.1520\n",
            "Epoch 39/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.4708 - accuracy: 0.1533\n",
            "Epoch 40/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.4619 - accuracy: 0.1535\n",
            "Epoch 41/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.4561 - accuracy: 0.1537\n",
            "Epoch 42/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.4504 - accuracy: 0.1541\n",
            "Epoch 43/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.4445 - accuracy: 0.1546\n",
            "Epoch 44/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.4388 - accuracy: 0.1549\n",
            "Epoch 45/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.4331 - accuracy: 0.1552\n",
            "Epoch 46/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.4252 - accuracy: 0.1555\n",
            "Epoch 47/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.4189 - accuracy: 0.1565\n",
            "Epoch 48/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.4167 - accuracy: 0.1569\n",
            "Epoch 49/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.4107 - accuracy: 0.1562\n",
            "Epoch 50/100\n",
            "2109/2109 [==============================] - 31s 15ms/step - loss: 5.4051 - accuracy: 0.1570\n",
            "Epoch 51/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.4002 - accuracy: 0.1576\n",
            "Epoch 52/100\n",
            "2109/2109 [==============================] - 31s 15ms/step - loss: 5.3971 - accuracy: 0.1580\n",
            "Epoch 53/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.3946 - accuracy: 0.1573\n",
            "Epoch 54/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.3886 - accuracy: 0.1588\n",
            "Epoch 55/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.3836 - accuracy: 0.1588\n",
            "Epoch 56/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.3855 - accuracy: 0.1582\n",
            "Epoch 57/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.3756 - accuracy: 0.1593\n",
            "Epoch 58/100\n",
            "2109/2109 [==============================] - 31s 15ms/step - loss: 5.3722 - accuracy: 0.1594\n",
            "Epoch 59/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.3714 - accuracy: 0.1595\n",
            "Epoch 60/100\n",
            "2109/2109 [==============================] - 31s 15ms/step - loss: 5.3653 - accuracy: 0.1594\n",
            "Epoch 61/100\n",
            "2109/2109 [==============================] - 31s 15ms/step - loss: 5.3630 - accuracy: 0.1599\n",
            "Epoch 62/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.3644 - accuracy: 0.1593\n",
            "Epoch 63/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.3558 - accuracy: 0.1596\n",
            "Epoch 64/100\n",
            "2109/2109 [==============================] - 31s 15ms/step - loss: 5.3561 - accuracy: 0.1602\n",
            "Epoch 65/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.3526 - accuracy: 0.1603\n",
            "Epoch 66/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.3511 - accuracy: 0.1603\n",
            "Epoch 67/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.3474 - accuracy: 0.1605\n",
            "Epoch 68/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.3434 - accuracy: 0.1604\n",
            "Epoch 69/100\n",
            "2109/2109 [==============================] - 31s 15ms/step - loss: 5.3428 - accuracy: 0.1610\n",
            "Epoch 70/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.3389 - accuracy: 0.1612\n",
            "Epoch 71/100\n",
            "2109/2109 [==============================] - 31s 15ms/step - loss: 5.3352 - accuracy: 0.1609\n",
            "Epoch 72/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.3379 - accuracy: 0.1608\n",
            "Epoch 73/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.3331 - accuracy: 0.1616\n",
            "Epoch 74/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.3274 - accuracy: 0.1613\n",
            "Epoch 75/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.3305 - accuracy: 0.1615\n",
            "Epoch 76/100\n",
            "2109/2109 [==============================] - 31s 15ms/step - loss: 5.3290 - accuracy: 0.1614\n",
            "Epoch 77/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.3258 - accuracy: 0.1614\n",
            "Epoch 78/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.3203 - accuracy: 0.1620\n",
            "Epoch 79/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.3217 - accuracy: 0.1616\n",
            "Epoch 80/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.3251 - accuracy: 0.1611\n",
            "Epoch 81/100\n",
            "2109/2109 [==============================] - 31s 15ms/step - loss: 5.3173 - accuracy: 0.1616\n",
            "Epoch 82/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.3180 - accuracy: 0.1616\n",
            "Epoch 83/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.3123 - accuracy: 0.1627\n",
            "Epoch 84/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 5.3140 - accuracy: 0.1622\n",
            "Epoch 85/100\n",
            "2109/2109 [==============================] - 30s 14ms/step - loss: 5.3106 - accuracy: 0.1622\n",
            "Epoch 86/100\n",
            "2109/2109 [==============================] - 30s 14ms/step - loss: 5.3148 - accuracy: 0.1621\n",
            "Epoch 87/100\n",
            "2109/2109 [==============================] - 30s 14ms/step - loss: 5.3089 - accuracy: 0.1627\n",
            "Epoch 88/100\n",
            "2109/2109 [==============================] - 30s 14ms/step - loss: 5.3098 - accuracy: 0.1621\n",
            "Epoch 89/100\n",
            "2109/2109 [==============================] - 30s 14ms/step - loss: 5.3061 - accuracy: 0.1627\n",
            "Epoch 90/100\n",
            "2109/2109 [==============================] - 30s 14ms/step - loss: 5.3079 - accuracy: 0.1624\n",
            "Epoch 91/100\n",
            "2109/2109 [==============================] - 30s 14ms/step - loss: 5.3070 - accuracy: 0.1627\n",
            "Epoch 92/100\n",
            "2109/2109 [==============================] - 30s 14ms/step - loss: 5.3026 - accuracy: 0.1628\n",
            "Epoch 93/100\n",
            "2109/2109 [==============================] - 30s 14ms/step - loss: 5.3060 - accuracy: 0.1624\n",
            "Epoch 94/100\n",
            "2109/2109 [==============================] - 30s 14ms/step - loss: 5.3022 - accuracy: 0.1630\n",
            "Epoch 95/100\n",
            "2109/2109 [==============================] - 30s 14ms/step - loss: 5.2996 - accuracy: 0.1625\n",
            "Epoch 96/100\n",
            "2109/2109 [==============================] - 30s 14ms/step - loss: 5.2997 - accuracy: 0.1625\n",
            "Epoch 97/100\n",
            "2109/2109 [==============================] - 30s 14ms/step - loss: 5.2979 - accuracy: 0.1627\n",
            "Epoch 98/100\n",
            "2109/2109 [==============================] - 30s 14ms/step - loss: 5.3018 - accuracy: 0.1627\n",
            "Epoch 99/100\n",
            "2109/2109 [==============================] - 30s 14ms/step - loss: 5.2951 - accuracy: 0.1629\n",
            "Epoch 100/100\n",
            "2109/2109 [==============================] - 30s 14ms/step - loss: 5.2963 - accuracy: 0.1625\n",
            "INFO:tensorflow:Assets written to: ram://4e8672ef-8c23-4c15-aab6-217eb9c6d242/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: ram://4e8672ef-8c23-4c15-aab6-217eb9c6d242/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Trying different phrases:\n",
        "## Hi how are you doing today sir\n",
        "# \"Raul was playing with his toy car\" --> 'Raul was playing with his toy car of the company is a share'\n",
        "# \"I think there has been an error\" --> \"I think there has been an error and the company s of the\"\n",
        "# \"I am a dual degree\" --> \"I am a dual degree i am a share of the\"\n",
        "# \"Data Science and Machine Learning\" --> \"Data Science and Machine Learning and the company s of the\"\n",
        "# \"I dont know where to start\" --> \"I dont know where to start i am a share of the\"\n",
        "# \"Artificial Intelligence is going to\" --> \"Artificial Intelligence is going to the company is a share of the company s\"\n",
        "test = ['Artificial Intelligence is going to']\n",
        "\n",
        "\n",
        "for t in test:\n",
        "    example = tokenizer.texts_to_sequences([t])\n",
        "    prediction = model.predict(np.array(example))\n",
        "    predicted_word = np.argmax(prediction)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))  # https://stackoverflow.com/a/43927939/246508\n",
        "    t1 = t + \" \" + reverse_word_map[predicted_word]\n",
        "\n",
        "    word2 = tokenizer.texts_to_sequences([t1])\n",
        "    prediction1 = model.predict(np.array(word2))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t2 = t1 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    word2 = tokenizer.texts_to_sequences([t2])\n",
        "    prediction1 = model.predict(np.array(word2))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t3 = t2 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    word2 = tokenizer.texts_to_sequences([t3])\n",
        "    prediction1 = model.predict(np.array(word2))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t4 = t3 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    word3 = tokenizer.texts_to_sequences([t4])\n",
        "    prediction1 = model.predict(np.array(word3))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t5 = t4 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    word4 = tokenizer.texts_to_sequences([t5])\n",
        "    prediction1 = model.predict(np.array(word4))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t6 = t5 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    word5 = tokenizer.texts_to_sequences([t6])\n",
        "    prediction1 = model.predict(np.array(word5))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t7 = t6 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    word6 = tokenizer.texts_to_sequences([t7])\n",
        "    prediction1 = model.predict(np.array(word6))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t8 = t7 + \" \" + reverse_word_map[predicted_word1]  \n",
        "\n",
        "    word7 = tokenizer.texts_to_sequences([t8])\n",
        "    prediction1 = model.predict(np.array(word7))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t9 = t8 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    print(t9)\n",
        "    #print (\"{0} -> {1}\".format(t, reverse_word_map[predicted_word]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMRw_eBlM3mf",
        "outputId": "7c40323b-2702-403c-ce3b-d2cd5a82f2ba"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artificial Intelligence is going to the company is a share of the company s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 3 performed even worse than model 2 (from ~0.4 accuracy to about 0.16. The text predictions don't seem to be making any sense either. "
      ],
      "metadata": {
        "id": "lps2mPudMuoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### Model 3: Model with one SimpleRNN layer and 200 nodes"
      ],
      "metadata": {
        "id": "dj6rrsGazcFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model 4: Model with one LSTM layer and one SimpleRNN layer with a dropout"
      ],
      "metadata": {
        "id": "iQDxre2COftb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  import numpy as np\n",
        "  from keras.preprocessing.text import Tokenizer\n",
        "  from tensorflow.keras.utils import to_categorical\n",
        "  from keras.models import Sequential\n",
        "  from keras.layers import Dense, Conv1D, Flatten\n",
        "  from keras.layers import LSTM, SimpleRNN\n",
        "  from keras.layers import Embedding, Dropout\n",
        "\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(sequences)\n",
        "  sequ = tokenizer.texts_to_sequences(sequences)\n",
        "\n",
        "  vocab_size = number_of_unique_tokens + 1\n",
        "\n",
        "  sequences0 = np.array(sequ)\n",
        "  X, y = sequences0[:,:-1], sequences0[:,-1]\n",
        "  y = to_categorical(y, num_classes=vocab_size)\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocab_size, sequence_length, input_length=sequence_length))\n",
        "  model.add(LSTM(200, return_sequences= True))\n",
        "  model.add(SimpleRNN(100))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(100, activation='relu'))\n",
        "  model.add(Dense(vocab_size, activation='softmax'))\n",
        "  \n",
        "  print(model.summary())\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "\n",
        "  model.fit(X, y, batch_size=80, epochs=100)\n",
        "  pickle.dump(model, open(path_in + \"model4.pkl\", \"wb\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgrUg9EHhMD-",
        "outputId": "0404b5a8-193c-4df8-b495-933c86db5bd7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 7, 7)              95718     \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 7, 200)            166400    \n",
            "                                                                 \n",
            " simple_rnn_1 (SimpleRNN)    (None, 100)               30100     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 100)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 13674)             1381074   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,683,392\n",
            "Trainable params: 1,683,392\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "2109/2109 [==============================] - 39s 17ms/step - loss: 7.3084 - accuracy: 0.0551\n",
            "Epoch 2/100\n",
            "2109/2109 [==============================] - 36s 17ms/step - loss: 7.1796 - accuracy: 0.0552\n",
            "Epoch 3/100\n",
            "2109/2109 [==============================] - 36s 17ms/step - loss: 7.0505 - accuracy: 0.0634\n",
            "Epoch 4/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 6.7984 - accuracy: 0.0804\n",
            "Epoch 5/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 6.6334 - accuracy: 0.0891\n",
            "Epoch 6/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 6.5002 - accuracy: 0.0989\n",
            "Epoch 7/100\n",
            "2109/2109 [==============================] - 36s 17ms/step - loss: 6.3746 - accuracy: 0.1076\n",
            "Epoch 8/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 6.2653 - accuracy: 0.1124\n",
            "Epoch 9/100\n",
            "2109/2109 [==============================] - 36s 17ms/step - loss: 6.1686 - accuracy: 0.1156\n",
            "Epoch 10/100\n",
            "2109/2109 [==============================] - 36s 17ms/step - loss: 6.0808 - accuracy: 0.1201\n",
            "Epoch 11/100\n",
            "2109/2109 [==============================] - 36s 17ms/step - loss: 6.0006 - accuracy: 0.1236\n",
            "Epoch 12/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 5.9245 - accuracy: 0.1277\n",
            "Epoch 13/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 5.8572 - accuracy: 0.1304\n",
            "Epoch 14/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 5.7962 - accuracy: 0.1332\n",
            "Epoch 15/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 5.7358 - accuracy: 0.1360\n",
            "Epoch 16/100\n",
            "2109/2109 [==============================] - 36s 17ms/step - loss: 5.6823 - accuracy: 0.1384\n",
            "Epoch 17/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 5.6334 - accuracy: 0.1403\n",
            "Epoch 18/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 5.5898 - accuracy: 0.1423\n",
            "Epoch 19/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 5.5473 - accuracy: 0.1447\n",
            "Epoch 20/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 5.5072 - accuracy: 0.1468\n",
            "Epoch 21/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 5.4708 - accuracy: 0.1485\n",
            "Epoch 22/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 5.4352 - accuracy: 0.1493\n",
            "Epoch 23/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 5.4009 - accuracy: 0.1507\n",
            "Epoch 24/100\n",
            "2109/2109 [==============================] - 36s 17ms/step - loss: 5.3681 - accuracy: 0.1522\n",
            "Epoch 25/100\n",
            "2109/2109 [==============================] - 36s 17ms/step - loss: 5.3379 - accuracy: 0.1534\n",
            "Epoch 26/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 5.3062 - accuracy: 0.1550\n",
            "Epoch 27/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 5.2839 - accuracy: 0.1561\n",
            "Epoch 28/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 5.2569 - accuracy: 0.1578\n",
            "Epoch 29/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 5.2300 - accuracy: 0.1585\n",
            "Epoch 30/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 5.2041 - accuracy: 0.1606\n",
            "Epoch 31/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 5.1827 - accuracy: 0.1618\n",
            "Epoch 32/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 5.1576 - accuracy: 0.1636\n",
            "Epoch 33/100\n",
            "2109/2109 [==============================] - 36s 17ms/step - loss: 5.1354 - accuracy: 0.1647\n",
            "Epoch 34/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 5.1174 - accuracy: 0.1659\n",
            "Epoch 35/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 5.0992 - accuracy: 0.1661\n",
            "Epoch 36/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 5.0760 - accuracy: 0.1674\n",
            "Epoch 37/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 5.0595 - accuracy: 0.1686\n",
            "Epoch 38/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 5.0418 - accuracy: 0.1692\n",
            "Epoch 39/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 5.0262 - accuracy: 0.1706\n",
            "Epoch 40/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 5.0066 - accuracy: 0.1720\n",
            "Epoch 41/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 4.9930 - accuracy: 0.1725\n",
            "Epoch 42/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 4.9775 - accuracy: 0.1739\n",
            "Epoch 43/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 4.9635 - accuracy: 0.1744\n",
            "Epoch 44/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 4.9486 - accuracy: 0.1753\n",
            "Epoch 45/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 4.9371 - accuracy: 0.1761\n",
            "Epoch 46/100\n",
            "2109/2109 [==============================] - 35s 16ms/step - loss: 4.9215 - accuracy: 0.1771\n",
            "Epoch 47/100\n",
            "2109/2109 [==============================] - 35s 16ms/step - loss: 4.9018 - accuracy: 0.1780\n",
            "Epoch 48/100\n",
            "2109/2109 [==============================] - 35s 16ms/step - loss: 4.8995 - accuracy: 0.1784\n",
            "Epoch 49/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 4.8813 - accuracy: 0.1797\n",
            "Epoch 50/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 4.8731 - accuracy: 0.1804\n",
            "Epoch 51/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 4.8588 - accuracy: 0.1813\n",
            "Epoch 52/100\n",
            "2109/2109 [==============================] - 35s 16ms/step - loss: 4.8509 - accuracy: 0.1811\n",
            "Epoch 53/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 4.8397 - accuracy: 0.1831\n",
            "Epoch 54/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 4.8266 - accuracy: 0.1836\n",
            "Epoch 55/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 4.8186 - accuracy: 0.1838\n",
            "Epoch 56/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 4.8078 - accuracy: 0.1850\n",
            "Epoch 57/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 4.7984 - accuracy: 0.1855\n",
            "Epoch 58/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 4.7876 - accuracy: 0.1863\n",
            "Epoch 59/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 4.7769 - accuracy: 0.1876\n",
            "Epoch 60/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 4.7690 - accuracy: 0.1873\n",
            "Epoch 61/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 4.7659 - accuracy: 0.1881\n",
            "Epoch 62/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 4.7554 - accuracy: 0.1894\n",
            "Epoch 63/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 4.7507 - accuracy: 0.1887\n",
            "Epoch 64/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 4.7413 - accuracy: 0.1895\n",
            "Epoch 65/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 4.7299 - accuracy: 0.1908\n",
            "Epoch 66/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 4.7247 - accuracy: 0.1910\n",
            "Epoch 67/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 4.7207 - accuracy: 0.1923\n",
            "Epoch 68/100\n",
            "2109/2109 [==============================] - 35s 16ms/step - loss: 4.7154 - accuracy: 0.1920\n",
            "Epoch 69/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 4.7035 - accuracy: 0.1923\n",
            "Epoch 70/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 4.6967 - accuracy: 0.1933\n",
            "Epoch 71/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 4.6896 - accuracy: 0.1937\n",
            "Epoch 72/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 4.6883 - accuracy: 0.1941\n",
            "Epoch 73/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 4.6814 - accuracy: 0.1944\n",
            "Epoch 74/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 4.6773 - accuracy: 0.1946\n",
            "Epoch 75/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 4.6653 - accuracy: 0.1956\n",
            "Epoch 76/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 4.6616 - accuracy: 0.1956\n",
            "Epoch 77/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 4.6548 - accuracy: 0.1967\n",
            "Epoch 78/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 4.6538 - accuracy: 0.1960\n",
            "Epoch 79/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 4.6416 - accuracy: 0.1978\n",
            "Epoch 80/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 4.6418 - accuracy: 0.1978\n",
            "Epoch 81/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 4.6320 - accuracy: 0.1993\n",
            "Epoch 82/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 4.6298 - accuracy: 0.1979\n",
            "Epoch 83/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 4.6248 - accuracy: 0.1990\n",
            "Epoch 84/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 4.6208 - accuracy: 0.1995\n",
            "Epoch 85/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 4.6127 - accuracy: 0.2000\n",
            "Epoch 86/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 4.6083 - accuracy: 0.1997\n",
            "Epoch 87/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 4.6078 - accuracy: 0.2003\n",
            "Epoch 88/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 4.6018 - accuracy: 0.2011\n",
            "Epoch 89/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 4.6022 - accuracy: 0.2005\n",
            "Epoch 90/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 4.5930 - accuracy: 0.2022\n",
            "Epoch 91/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 4.5923 - accuracy: 0.2019\n",
            "Epoch 92/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 4.5885 - accuracy: 0.2028\n",
            "Epoch 93/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 4.5824 - accuracy: 0.2022\n",
            "Epoch 94/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 4.5786 - accuracy: 0.2032\n",
            "Epoch 95/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 4.5724 - accuracy: 0.2025\n",
            "Epoch 96/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 4.5717 - accuracy: 0.2032\n",
            "Epoch 97/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 4.5709 - accuracy: 0.2037\n",
            "Epoch 98/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 4.5662 - accuracy: 0.2042\n",
            "Epoch 99/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 4.5601 - accuracy: 0.2046\n",
            "Epoch 100/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 4.5573 - accuracy: 0.2045\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: ram://99e9752c-e36c-481e-a897-89dcbdd6beb3/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: ram://99e9752c-e36c-481e-a897-89dcbdd6beb3/assets\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fc9f74b6910> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Trying different phrases:\n",
        "## Hi how are you doing today\n",
        "# \"Raul was playing with his toy car\" --> 'Raul was playing with his toy car in the company the company said'\n",
        "# \"I think there has been an error\" --> \"I think there has been an error in the interview the company s\"\n",
        "# \"I am a dual degree\" --> \"I am a dual degree corps the company s the editorial\"\n",
        "# \"Data Science and Machine Learning\" --> \"Data Science and Machine Learning tracking of the company s mother\"\n",
        "# \"I dont know where to start\" --> \"I dont know where to start the other market the company said\"\n",
        "# \"Artificial Intelligence is going to\" --> \"Artificial Intelligence is going to the two bloc of the u\"\n",
        "test = ['Hi how are you doing today']\n",
        "\n",
        "\n",
        "for t in test:\n",
        "    example = tokenizer.texts_to_sequences([t])\n",
        "    prediction = model.predict(np.array(example))\n",
        "    predicted_word = np.argmax(prediction)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))  # https://stackoverflow.com/a/43927939/246508\n",
        "    t1 = t + \" \" + reverse_word_map[predicted_word]\n",
        "\n",
        "    word2 = tokenizer.texts_to_sequences([t1])\n",
        "    prediction1 = model.predict(np.array(word2))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t2 = t1 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    word2 = tokenizer.texts_to_sequences([t2])\n",
        "    prediction1 = model.predict(np.array(word2))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t3 = t2 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    word2 = tokenizer.texts_to_sequences([t3])\n",
        "    prediction1 = model.predict(np.array(word2))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t4 = t3 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    word3 = tokenizer.texts_to_sequences([t4])\n",
        "    prediction1 = model.predict(np.array(word3))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t5 = t4 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    word4 = tokenizer.texts_to_sequences([t5])\n",
        "    prediction1 = model.predict(np.array(word4))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t6 = t5 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    word5 = tokenizer.texts_to_sequences([t6])\n",
        "    prediction1 = model.predict(np.array(word5))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t7 = t6 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    word6 = tokenizer.texts_to_sequences([t7])\n",
        "    prediction1 = model.predict(np.array(word6))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t8 = t7 + \" \" + reverse_word_map[predicted_word1]  \n",
        "\n",
        "    word7 = tokenizer.texts_to_sequences([t8])\n",
        "    prediction1 = model.predict(np.array(word7))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t9 = t8 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    print(t6)\n",
        "    #print (\"{0} -> {1}\".format(t, reverse_word_map[predicted_word]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PehOwnTRfHW7",
        "outputId": "c1dcc346-18b4-48d4-cee0-50f6d8eecad0"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi how are you doing today the company s is t 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 4 performed better than model 3, but still was not as good as model 1 and model 2."
      ],
      "metadata": {
        "id": "AGDZhmvsfILd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model 5: Model with one LSTM layer with 600 nodes"
      ],
      "metadata": {
        "id": "rlJ-gKHBgORW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  import numpy as np\n",
        "  from keras.preprocessing.text import Tokenizer\n",
        "  from tensorflow.keras.utils import to_categorical\n",
        "  from keras.models import Sequential\n",
        "  from keras.layers import Dense, Conv1D, Flatten\n",
        "  from keras.layers import LSTM, SimpleRNN\n",
        "  from keras.layers import Embedding, Dropout\n",
        "\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(sequences)\n",
        "  sequ = tokenizer.texts_to_sequences(sequences)\n",
        "\n",
        "  vocab_size = number_of_unique_tokens + 1\n",
        "\n",
        "  sequences0 = np.array(sequ)\n",
        "  X, y = sequences0[:,:-1], sequences0[:,-1]\n",
        "  y = to_categorical(y, num_classes=vocab_size)\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocab_size, sequence_length, input_length=sequence_length))\n",
        "  model.add(LSTM(600))\n",
        "  model.add(Dense(100, activation='relu'))\n",
        "  model.add(Dense(vocab_size, activation='softmax'))\n",
        "  \n",
        "  print(model.summary())\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "\n",
        "  model.fit(X, y, batch_size=80, epochs=100)\n",
        "  pickle.dump(model, open(path_in + \"model5.pkl\", \"wb\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Aq4wDehhMGT",
        "outputId": "39dbf5b5-a80d-4922-bfbe-7899d67618a2"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 7, 7)              95718     \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 600)               1459200   \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 100)               60100     \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 13674)             1381074   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,996,092\n",
            "Trainable params: 2,996,092\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "2109/2109 [==============================] - 30s 13ms/step - loss: 7.1745 - accuracy: 0.0605\n",
            "Epoch 2/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 6.7471 - accuracy: 0.0862\n",
            "Epoch 3/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 6.4048 - accuracy: 0.1122\n",
            "Epoch 4/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 6.0886 - accuracy: 0.1283\n",
            "Epoch 5/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 5.8154 - accuracy: 0.1385\n",
            "Epoch 6/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 5.5787 - accuracy: 0.1468\n",
            "Epoch 7/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 5.3543 - accuracy: 0.1568\n",
            "Epoch 8/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 5.1283 - accuracy: 0.1658\n",
            "Epoch 9/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 4.8981 - accuracy: 0.1753\n",
            "Epoch 10/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 4.6594 - accuracy: 0.1872\n",
            "Epoch 11/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 4.4101 - accuracy: 0.2035\n",
            "Epoch 12/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 4.1619 - accuracy: 0.2256\n",
            "Epoch 13/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 3.9172 - accuracy: 0.2518\n",
            "Epoch 14/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 3.6793 - accuracy: 0.2820\n",
            "Epoch 15/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 3.4520 - accuracy: 0.3104\n",
            "Epoch 16/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 3.2331 - accuracy: 0.3410\n",
            "Epoch 17/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 3.0252 - accuracy: 0.3721\n",
            "Epoch 18/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 2.8272 - accuracy: 0.4015\n",
            "Epoch 19/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 2.6415 - accuracy: 0.4315\n",
            "Epoch 20/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 2.4620 - accuracy: 0.4605\n",
            "Epoch 21/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 2.2989 - accuracy: 0.4887\n",
            "Epoch 22/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 2.1409 - accuracy: 0.5181\n",
            "Epoch 23/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 1.9989 - accuracy: 0.5426\n",
            "Epoch 24/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 1.8667 - accuracy: 0.5678\n",
            "Epoch 25/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 1.7330 - accuracy: 0.5943\n",
            "Epoch 26/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 1.6240 - accuracy: 0.6152\n",
            "Epoch 27/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 1.5148 - accuracy: 0.6370\n",
            "Epoch 28/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 1.4153 - accuracy: 0.6586\n",
            "Epoch 29/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 1.3304 - accuracy: 0.6738\n",
            "Epoch 30/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 1.2440 - accuracy: 0.6925\n",
            "Epoch 31/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 1.1748 - accuracy: 0.7053\n",
            "Epoch 32/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 1.1084 - accuracy: 0.7213\n",
            "Epoch 33/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 1.0376 - accuracy: 0.7354\n",
            "Epoch 34/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.9884 - accuracy: 0.7458\n",
            "Epoch 35/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.9221 - accuracy: 0.7611\n",
            "Epoch 36/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.8861 - accuracy: 0.7680\n",
            "Epoch 37/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.8393 - accuracy: 0.7802\n",
            "Epoch 38/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.7927 - accuracy: 0.7901\n",
            "Epoch 39/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.7622 - accuracy: 0.7966\n",
            "Epoch 40/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.7332 - accuracy: 0.8032\n",
            "Epoch 41/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.6969 - accuracy: 0.8121\n",
            "Epoch 42/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.6791 - accuracy: 0.8149\n",
            "Epoch 43/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.6429 - accuracy: 0.8236\n",
            "Epoch 44/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.6266 - accuracy: 0.8291\n",
            "Epoch 45/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.5967 - accuracy: 0.8355\n",
            "Epoch 46/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.5771 - accuracy: 0.8384\n",
            "Epoch 47/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.5666 - accuracy: 0.8424\n",
            "Epoch 48/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.5450 - accuracy: 0.8475\n",
            "Epoch 49/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.5337 - accuracy: 0.8509\n",
            "Epoch 50/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.5126 - accuracy: 0.8557\n",
            "Epoch 51/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.5059 - accuracy: 0.8563\n",
            "Epoch 52/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.4780 - accuracy: 0.8651\n",
            "Epoch 53/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.4793 - accuracy: 0.8638\n",
            "Epoch 54/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.4646 - accuracy: 0.8676\n",
            "Epoch 55/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.4566 - accuracy: 0.8696\n",
            "Epoch 56/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.4470 - accuracy: 0.8724\n",
            "Epoch 57/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.4399 - accuracy: 0.8735\n",
            "Epoch 58/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.4235 - accuracy: 0.8778\n",
            "Epoch 59/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.4181 - accuracy: 0.8796\n",
            "Epoch 60/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.4114 - accuracy: 0.8822\n",
            "Epoch 61/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.4163 - accuracy: 0.8810\n",
            "Epoch 62/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.4004 - accuracy: 0.8851\n",
            "Epoch 63/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.3926 - accuracy: 0.8866\n",
            "Epoch 64/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.3863 - accuracy: 0.8888\n",
            "Epoch 65/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.3875 - accuracy: 0.8883\n",
            "Epoch 66/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.3679 - accuracy: 0.8942\n",
            "Epoch 67/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.3767 - accuracy: 0.8914\n",
            "Epoch 68/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.3662 - accuracy: 0.8948\n",
            "Epoch 69/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.3622 - accuracy: 0.8957\n",
            "Epoch 70/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.3602 - accuracy: 0.8959\n",
            "Epoch 71/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.3498 - accuracy: 0.8981\n",
            "Epoch 72/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.3363 - accuracy: 0.9022\n",
            "Epoch 73/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.3545 - accuracy: 0.8974\n",
            "Epoch 74/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.3343 - accuracy: 0.9033\n",
            "Epoch 75/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.3381 - accuracy: 0.9018\n",
            "Epoch 76/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.3344 - accuracy: 0.9032\n",
            "Epoch 77/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.3224 - accuracy: 0.9067\n",
            "Epoch 78/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.3248 - accuracy: 0.9052\n",
            "Epoch 79/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.3220 - accuracy: 0.9075\n",
            "Epoch 80/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.3231 - accuracy: 0.9065\n",
            "Epoch 81/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.3143 - accuracy: 0.9095\n",
            "Epoch 82/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.3077 - accuracy: 0.9114\n",
            "Epoch 83/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.3156 - accuracy: 0.9088\n",
            "Epoch 84/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.3100 - accuracy: 0.9100\n",
            "Epoch 85/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.2973 - accuracy: 0.9143\n",
            "Epoch 86/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.3084 - accuracy: 0.9107\n",
            "Epoch 87/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.3026 - accuracy: 0.9126\n",
            "Epoch 88/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.3002 - accuracy: 0.9133\n",
            "Epoch 89/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.2931 - accuracy: 0.9152\n",
            "Epoch 90/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.2851 - accuracy: 0.9168\n",
            "Epoch 91/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.2916 - accuracy: 0.9154\n",
            "Epoch 92/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.2968 - accuracy: 0.9147\n",
            "Epoch 93/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.2789 - accuracy: 0.9191\n",
            "Epoch 94/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.2853 - accuracy: 0.9181\n",
            "Epoch 95/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.2885 - accuracy: 0.9170\n",
            "Epoch 96/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.2761 - accuracy: 0.9202\n",
            "Epoch 97/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.2766 - accuracy: 0.9197\n",
            "Epoch 98/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.2733 - accuracy: 0.9210\n",
            "Epoch 99/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.2688 - accuracy: 0.9229\n",
            "Epoch 100/100\n",
            "2109/2109 [==============================] - 27s 13ms/step - loss: 0.2755 - accuracy: 0.9200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: ram://12d87985-5355-4445-95b1-4ab8d24fd32d/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: ram://12d87985-5355-4445-95b1-4ab8d24fd32d/assets\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd320239e50> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Trying different phrases:\n",
        "## Hi how are you doing today sir\n",
        "# 'I am so happy that' --> ''I am so happy that I am able to understand her'\n",
        "# \"I think that he\" --> \"I think that he might get sure new two arms\"\n",
        "# 'I am a student at' --> \"I am a student at boston university i aim to be\"\n",
        "# \"Raul was playing with his toy car\" --> 'Raul was playing with his toy car in the company the company said'\n",
        "# \"I think there has been an error\" --> \"I think there has been an error in the interview the company s\"\n",
        "# \"I am a dual degree\" --> \"I am a dual degree corps the company s the editorial\"\n",
        "# \"Data Science and Machine Learning\" --> \"Data Science and Machine Learning tracking of the company s mother\"\n",
        "# \"I dont know where to start\" --> \"I dont know where to start the other market the company said\"\n",
        "# \"Artificial Intelligence is going to\" --> \"Artificial Intelligence is going to the two bloc of the u\"\n",
        "test = ['I think that I ']\n",
        "\n",
        "\n",
        "for t in test:\n",
        "    example = tokenizer.texts_to_sequences([t])\n",
        "    prediction = model.predict(np.array(example))\n",
        "    predicted_word = np.argmax(prediction)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))  # https://stackoverflow.com/a/43927939/246508\n",
        "    t1 = t + \" \" + reverse_word_map[predicted_word]\n",
        "\n",
        "    word2 = tokenizer.texts_to_sequences([t1])\n",
        "    prediction1 = model.predict(np.array(word2))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t2 = t1 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    word2 = tokenizer.texts_to_sequences([t2])\n",
        "    prediction1 = model.predict(np.array(word2))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t3 = t2 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    word2 = tokenizer.texts_to_sequences([t3])\n",
        "    prediction1 = model.predict(np.array(word2))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t4 = t3 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    word3 = tokenizer.texts_to_sequences([t4])\n",
        "    prediction1 = model.predict(np.array(word3))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t5 = t4 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    word4 = tokenizer.texts_to_sequences([t5])\n",
        "    prediction1 = model.predict(np.array(word4))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t6 = t5 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    word5 = tokenizer.texts_to_sequences([t6])\n",
        "    prediction1 = model.predict(np.array(word5))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t7 = t6 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    word6 = tokenizer.texts_to_sequences([t7])\n",
        "    prediction1 = model.predict(np.array(word6))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t8 = t7 + \" \" + reverse_word_map[predicted_word1]  \n",
        "\n",
        "    word7 = tokenizer.texts_to_sequences([t8])\n",
        "    prediction1 = model.predict(np.array(word7))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t9 = t8 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    print(t6)\n",
        "    #print (\"{0} -> {1}\".format(t, reverse_word_map[predicted_word]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuSIpgTpfbHz",
        "outputId": "f327a42a-291a-40a7-f737-347b9a6f304b"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I think that I  m not challenging sure to be\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 5 performed really well- a lot better than all other models. It scored a validation accuracy of 0.92 and predicted the text with better accuracy- the output made a lot more sense in most of the cases. For example, when I input \"I am so happy that\" it returned \"I am so happy that I am able to understand her\" which is a legit English sentence with proper grammar and meaning. When I input \"I am a student at\", the model output \"I am a student at boston university i aim to be\". This is very interesting because I completed my undergrad at Boston University; it seems like the model has been learning my style and my writings fairly well."
      ],
      "metadata": {
        "id": "45xFjSQJvtPP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model 6: Model with one LSTM layer with 800 nodes"
      ],
      "metadata": {
        "id": "NsdrmAeUu16w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  import numpy as np\n",
        "  from keras.preprocessing.text import Tokenizer\n",
        "  from tensorflow.keras.utils import to_categorical\n",
        "  from keras.models import Sequential\n",
        "  from keras.layers import Dense, Conv1D, Flatten\n",
        "  from keras.layers import LSTM, SimpleRNN\n",
        "  from keras.layers import Embedding, Dropout\n",
        "\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(sequences)\n",
        "  sequ = tokenizer.texts_to_sequences(sequences)\n",
        "\n",
        "  vocab_size = number_of_unique_tokens + 1\n",
        "\n",
        "  sequences0 = np.array(sequ)\n",
        "  X, y = sequences0[:,:-1], sequences0[:,-1]\n",
        "  y = to_categorical(y, num_classes=vocab_size)\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocab_size, sequence_length, input_length=sequence_length))\n",
        "  model.add(LSTM(800))\n",
        "  model.add(Dense(100, activation='relu'))\n",
        "  model.add(Dense(vocab_size, activation='softmax'))\n",
        "  \n",
        "  print(model.summary())\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "\n",
        "  model.fit(X, y, batch_size=80, epochs=100)\n",
        "  pickle.dump(model, open(path_in + \"model6.pkl\", \"wb\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgBktR0rfbKo",
        "outputId": "7c97c300-e4c5-430d-9cfa-b34c34cceebe"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_4 (Embedding)     (None, 7, 7)              95718     \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, 800)               2585600   \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 100)               80100     \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 13674)             1381074   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,142,492\n",
            "Trainable params: 4,142,492\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "2109/2109 [==============================] - 32s 14ms/step - loss: 7.1781 - accuracy: 0.0594\n",
            "Epoch 2/100\n",
            "2109/2109 [==============================] - 29s 14ms/step - loss: 6.7780 - accuracy: 0.0825\n",
            "Epoch 3/100\n",
            "2109/2109 [==============================] - 28s 14ms/step - loss: 6.4598 - accuracy: 0.1068\n",
            "Epoch 4/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 6.2019 - accuracy: 0.1220\n",
            "Epoch 5/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 5.9581 - accuracy: 0.1330\n",
            "Epoch 6/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 5.7305 - accuracy: 0.1414\n",
            "Epoch 7/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 5.5132 - accuracy: 0.1501\n",
            "Epoch 8/100\n",
            "2109/2109 [==============================] - 29s 14ms/step - loss: 5.3052 - accuracy: 0.1584\n",
            "Epoch 9/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 5.1006 - accuracy: 0.1664\n",
            "Epoch 10/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 4.8954 - accuracy: 0.1757\n",
            "Epoch 11/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 4.6826 - accuracy: 0.1874\n",
            "Epoch 12/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 4.4570 - accuracy: 0.2015\n",
            "Epoch 13/100\n",
            "2109/2109 [==============================] - 29s 14ms/step - loss: 4.2122 - accuracy: 0.2232\n",
            "Epoch 14/100\n",
            "2109/2109 [==============================] - 28s 14ms/step - loss: 3.9579 - accuracy: 0.2506\n",
            "Epoch 15/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 3.6929 - accuracy: 0.2829\n",
            "Epoch 16/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 3.4297 - accuracy: 0.3173\n",
            "Epoch 17/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 3.1732 - accuracy: 0.3523\n",
            "Epoch 18/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 2.9174 - accuracy: 0.3899\n",
            "Epoch 19/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 2.6783 - accuracy: 0.4271\n",
            "Epoch 20/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 2.4459 - accuracy: 0.4654\n",
            "Epoch 21/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 2.2280 - accuracy: 0.5018\n",
            "Epoch 22/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 2.0261 - accuracy: 0.5384\n",
            "Epoch 23/100\n",
            "2109/2109 [==============================] - 29s 14ms/step - loss: 1.8396 - accuracy: 0.5738\n",
            "Epoch 24/100\n",
            "2109/2109 [==============================] - 29s 14ms/step - loss: 1.6724 - accuracy: 0.6047\n",
            "Epoch 25/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 1.5120 - accuracy: 0.6364\n",
            "Epoch 26/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 1.3784 - accuracy: 0.6639\n",
            "Epoch 27/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 1.2492 - accuracy: 0.6912\n",
            "Epoch 28/100\n",
            "2109/2109 [==============================] - 29s 14ms/step - loss: 1.1352 - accuracy: 0.7153\n",
            "Epoch 29/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 1.0395 - accuracy: 0.7362\n",
            "Epoch 30/100\n",
            "2109/2109 [==============================] - 29s 14ms/step - loss: 0.9542 - accuracy: 0.7536\n",
            "Epoch 31/100\n",
            "2109/2109 [==============================] - 29s 14ms/step - loss: 0.8674 - accuracy: 0.7736\n",
            "Epoch 32/100\n",
            "2109/2109 [==============================] - 29s 14ms/step - loss: 0.8084 - accuracy: 0.7879\n",
            "Epoch 33/100\n",
            "2109/2109 [==============================] - 29s 14ms/step - loss: 0.7440 - accuracy: 0.8013\n",
            "Epoch 34/100\n",
            "2109/2109 [==============================] - 29s 14ms/step - loss: 0.6908 - accuracy: 0.8142\n",
            "Epoch 35/100\n",
            "2109/2109 [==============================] - 29s 14ms/step - loss: 0.6532 - accuracy: 0.8229\n",
            "Epoch 36/100\n",
            "2109/2109 [==============================] - 29s 14ms/step - loss: 0.6108 - accuracy: 0.8338\n",
            "Epoch 37/100\n",
            "2109/2109 [==============================] - 29s 14ms/step - loss: 0.5796 - accuracy: 0.8410\n",
            "Epoch 38/100\n",
            "2109/2109 [==============================] - 28s 14ms/step - loss: 0.5503 - accuracy: 0.8474\n",
            "Epoch 39/100\n",
            "2109/2109 [==============================] - 29s 14ms/step - loss: 0.5172 - accuracy: 0.8568\n",
            "Epoch 40/100\n",
            "2109/2109 [==============================] - 29s 14ms/step - loss: 0.4958 - accuracy: 0.8610\n",
            "Epoch 41/100\n",
            "2109/2109 [==============================] - 28s 14ms/step - loss: 0.4770 - accuracy: 0.8660\n",
            "Epoch 42/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.4628 - accuracy: 0.8704\n",
            "Epoch 43/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.4467 - accuracy: 0.8745\n",
            "Epoch 44/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.4266 - accuracy: 0.8784\n",
            "Epoch 45/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.4172 - accuracy: 0.8811\n",
            "Epoch 46/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.4025 - accuracy: 0.8849\n",
            "Epoch 47/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.3871 - accuracy: 0.8896\n",
            "Epoch 48/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.3828 - accuracy: 0.8901\n",
            "Epoch 49/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.3744 - accuracy: 0.8932\n",
            "Epoch 50/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.3652 - accuracy: 0.8951\n",
            "Epoch 51/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.3546 - accuracy: 0.8978\n",
            "Epoch 52/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.3569 - accuracy: 0.8975\n",
            "Epoch 53/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.3423 - accuracy: 0.9025\n",
            "Epoch 54/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.3401 - accuracy: 0.9023\n",
            "Epoch 55/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.3214 - accuracy: 0.9076\n",
            "Epoch 56/100\n",
            "2109/2109 [==============================] - 29s 14ms/step - loss: 0.3388 - accuracy: 0.9028\n",
            "Epoch 57/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.3129 - accuracy: 0.9098\n",
            "Epoch 58/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.3174 - accuracy: 0.9101\n",
            "Epoch 59/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.3170 - accuracy: 0.9081\n",
            "Epoch 60/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.3079 - accuracy: 0.9107\n",
            "Epoch 61/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.3026 - accuracy: 0.9133\n",
            "Epoch 62/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.2954 - accuracy: 0.9153\n",
            "Epoch 63/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.2955 - accuracy: 0.9148\n",
            "Epoch 64/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.2934 - accuracy: 0.9163\n",
            "Epoch 65/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.2908 - accuracy: 0.9160\n",
            "Epoch 66/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.2832 - accuracy: 0.9182\n",
            "Epoch 67/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.2812 - accuracy: 0.9193\n",
            "Epoch 68/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.2787 - accuracy: 0.9201\n",
            "Epoch 69/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.2726 - accuracy: 0.9205\n",
            "Epoch 70/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.2746 - accuracy: 0.9203\n",
            "Epoch 71/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.2692 - accuracy: 0.9215\n",
            "Epoch 72/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.2650 - accuracy: 0.9237\n",
            "Epoch 73/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.2657 - accuracy: 0.9233\n",
            "Epoch 74/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.2615 - accuracy: 0.9238\n",
            "Epoch 75/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.2588 - accuracy: 0.9256\n",
            "Epoch 76/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.2630 - accuracy: 0.9235\n",
            "Epoch 77/100\n",
            "2109/2109 [==============================] - 29s 14ms/step - loss: 0.2504 - accuracy: 0.9276\n",
            "Epoch 78/100\n",
            "2109/2109 [==============================] - 28s 14ms/step - loss: 0.2523 - accuracy: 0.9272\n",
            "Epoch 79/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.2509 - accuracy: 0.9267\n",
            "Epoch 80/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.2428 - accuracy: 0.9298\n",
            "Epoch 81/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.2491 - accuracy: 0.9283\n",
            "Epoch 82/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.2446 - accuracy: 0.9291\n",
            "Epoch 83/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.2380 - accuracy: 0.9311\n",
            "Epoch 84/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.2400 - accuracy: 0.9310\n",
            "Epoch 85/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.2418 - accuracy: 0.9300\n",
            "Epoch 86/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.2336 - accuracy: 0.9323\n",
            "Epoch 87/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.2274 - accuracy: 0.9342\n",
            "Epoch 88/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.2353 - accuracy: 0.9326\n",
            "Epoch 89/100\n",
            "2109/2109 [==============================] - 29s 14ms/step - loss: 0.2355 - accuracy: 0.9324\n",
            "Epoch 90/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.2238 - accuracy: 0.9346\n",
            "Epoch 91/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.2236 - accuracy: 0.9357\n",
            "Epoch 92/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.2279 - accuracy: 0.9345\n",
            "Epoch 93/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.2199 - accuracy: 0.9362\n",
            "Epoch 94/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.2274 - accuracy: 0.9349\n",
            "Epoch 95/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.2191 - accuracy: 0.9373\n",
            "Epoch 96/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.2151 - accuracy: 0.9371\n",
            "Epoch 97/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.2212 - accuracy: 0.9364\n",
            "Epoch 98/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.2110 - accuracy: 0.9394\n",
            "Epoch 99/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.2215 - accuracy: 0.9365\n",
            "Epoch 100/100\n",
            "2109/2109 [==============================] - 28s 13ms/step - loss: 0.2109 - accuracy: 0.9395\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_4_layer_call_fn, lstm_cell_4_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: ram://898a3a33-73b7-4a24-a9b7-ffa6e13ce13b/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: ram://898a3a33-73b7-4a24-a9b7-ffa6e13ce13b/assets\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fc9f77ded90> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Trying different phrases:\n",
        "## Hi how are you doing today sir\n",
        "# 'I am so happy that' --> ''I am so happy that I am able to understand her'\n",
        "# \"I think that he\" --> \"I think that he might get sure new two arms\"\n",
        "# 'I am a student at' --> \"I am a student at boston university i aim to be\"\n",
        "# \"Raul was playing with his toy car\" --> 'Raul was playing with his toy car in the company the company said'\n",
        "# \"I think there has been an error\" --> \"I think there has been an error in the interview the company s\"\n",
        "# \"I am a dual degree\" --> \"I am a dual degree corps the company s the editorial\"\n",
        "# \"Data Science and Machine Learning\" --> \"Data Science and Machine Learning tracking of the company s mother\"\n",
        "# \"I dont know where to start\" --> \"I dont know where to start the other market the company said\"\n",
        "# \"Artificial Intelligence is going to\" --> \"Artificial Intelligence is going to the two bloc of the u\"\n",
        "test = ['why is the belt below the buckle']\n",
        "\n",
        "\n",
        "for t in test:\n",
        "    example = tokenizer.texts_to_sequences([t])\n",
        "    prediction = model.predict(np.array(example))\n",
        "    predicted_word = np.argmax(prediction)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))  # https://stackoverflow.com/a/43927939/246508\n",
        "    t1 = t + \" \" + reverse_word_map[predicted_word]\n",
        "\n",
        "    word2 = tokenizer.texts_to_sequences([t1])\n",
        "    prediction1 = model.predict(np.array(word2))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t2 = t1 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    word2 = tokenizer.texts_to_sequences([t2])\n",
        "    prediction1 = model.predict(np.array(word2))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t3 = t2 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    word2 = tokenizer.texts_to_sequences([t3])\n",
        "    prediction1 = model.predict(np.array(word2))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t4 = t3 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    word3 = tokenizer.texts_to_sequences([t4])\n",
        "    prediction1 = model.predict(np.array(word3))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t5 = t4 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    word4 = tokenizer.texts_to_sequences([t5])\n",
        "    prediction1 = model.predict(np.array(word4))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t6 = t5 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    word5 = tokenizer.texts_to_sequences([t6])\n",
        "    prediction1 = model.predict(np.array(word5))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t7 = t6 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    word6 = tokenizer.texts_to_sequences([t7])\n",
        "    prediction1 = model.predict(np.array(word6))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t8 = t7 + \" \" + reverse_word_map[predicted_word1]  \n",
        "\n",
        "    word7 = tokenizer.texts_to_sequences([t8])\n",
        "    prediction1 = model.predict(np.array(word7))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t9 = t8 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    print(t6)\n",
        "    #print (\"{0} -> {1}\".format(t, reverse_word_map[predicted_word]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fNRI86WfbNe",
        "outputId": "a419444b-d07d-4be3-9da8-c289842e1642"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "why is the belt below the buckle with the good level of the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model 7: One LSTM layer with 1000 nodes and one fully-connected layer with 500 nodes"
      ],
      "metadata": {
        "id": "OgA0-Rh-_b7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  import numpy as np\n",
        "  from keras.preprocessing.text import Tokenizer\n",
        "  from tensorflow.keras.utils import to_categorical\n",
        "  from keras.models import Sequential\n",
        "  from keras.layers import Dense, Conv1D, Flatten\n",
        "  from keras.layers import LSTM, SimpleRNN\n",
        "  from keras.layers import Embedding, Dropout\n",
        "\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(sequences)\n",
        "  sequ = tokenizer.texts_to_sequences(sequences)\n",
        "\n",
        "  vocab_size = number_of_unique_tokens + 1\n",
        "\n",
        "  sequences0 = np.array(sequ)\n",
        "  X, y = sequences0[:,:-1], sequences0[:,-1]\n",
        "  y = to_categorical(y, num_classes=vocab_size)\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocab_size, sequence_length, input_length=sequence_length))\n",
        "  model.add(LSTM(1000))\n",
        "  model.add(Dense(500, activation='relu'))\n",
        "  model.add(Dense(vocab_size, activation='softmax'))\n",
        "  \n",
        "  print(model.summary())\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "\n",
        "  model.fit(X, y, batch_size=80, epochs=100)\n",
        "  pickle.dump(model, open(path_in + \"model7.pkl\", \"wb\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9JYp-nLfbQF",
        "outputId": "28a62c98-2234-4174-d63d-c1d14099caae"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_6 (Embedding)     (None, 7, 7)              95718     \n",
            "                                                                 \n",
            " lstm_6 (LSTM)               (None, 1000)              4032000   \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 500)               500500    \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 13674)             6850674   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11,478,892\n",
            "Trainable params: 11,478,892\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "2109/2109 [==============================] - 38s 17ms/step - loss: 7.1035 - accuracy: 0.0674\n",
            "Epoch 2/100\n",
            "2109/2109 [==============================] - 35s 17ms/step - loss: 6.5309 - accuracy: 0.1088\n",
            "Epoch 3/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 6.1254 - accuracy: 0.1296\n",
            "Epoch 4/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 5.7773 - accuracy: 0.1435\n",
            "Epoch 5/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 5.4536 - accuracy: 0.1574\n",
            "Epoch 6/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 5.1277 - accuracy: 0.1691\n",
            "Epoch 7/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 4.7880 - accuracy: 0.1848\n",
            "Epoch 8/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 4.4343 - accuracy: 0.2044\n",
            "Epoch 9/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 4.0792 - accuracy: 0.2361\n",
            "Epoch 10/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 3.7323 - accuracy: 0.2741\n",
            "Epoch 11/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 3.3954 - accuracy: 0.3146\n",
            "Epoch 12/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 3.0632 - accuracy: 0.3592\n",
            "Epoch 13/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 2.7233 - accuracy: 0.4110\n",
            "Epoch 14/100\n",
            "2109/2109 [==============================] - 33s 16ms/step - loss: 2.3718 - accuracy: 0.4678\n",
            "Epoch 15/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 2.0196 - accuracy: 0.5306\n",
            "Epoch 16/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 1.6768 - accuracy: 0.5983\n",
            "Epoch 17/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 1.3633 - accuracy: 0.6629\n",
            "Epoch 18/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 1.0879 - accuracy: 0.7241\n",
            "Epoch 19/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 0.8545 - accuracy: 0.7770\n",
            "Epoch 20/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 0.6738 - accuracy: 0.8222\n",
            "Epoch 21/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 0.5310 - accuracy: 0.8585\n",
            "Epoch 22/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 0.4283 - accuracy: 0.8839\n",
            "Epoch 23/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 0.3478 - accuracy: 0.9047\n",
            "Epoch 24/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 0.2957 - accuracy: 0.9189\n",
            "Epoch 25/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 0.2539 - accuracy: 0.9304\n",
            "Epoch 26/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 0.2279 - accuracy: 0.9379\n",
            "Epoch 27/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 0.2081 - accuracy: 0.9428\n",
            "Epoch 28/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 0.1880 - accuracy: 0.9479\n",
            "Epoch 29/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 0.1747 - accuracy: 0.9529\n",
            "Epoch 30/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 0.1676 - accuracy: 0.9530\n",
            "Epoch 31/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 0.1545 - accuracy: 0.9578\n",
            "Epoch 32/100\n",
            "2109/2109 [==============================] - 33s 16ms/step - loss: 0.1511 - accuracy: 0.9580\n",
            "Epoch 33/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 0.1415 - accuracy: 0.9610\n",
            "Epoch 34/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 0.1322 - accuracy: 0.9629\n",
            "Epoch 35/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 0.1286 - accuracy: 0.9641\n",
            "Epoch 36/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 0.1289 - accuracy: 0.9636\n",
            "Epoch 37/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 0.1211 - accuracy: 0.9661\n",
            "Epoch 38/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 0.1140 - accuracy: 0.9681\n",
            "Epoch 39/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 0.1167 - accuracy: 0.9668\n",
            "Epoch 40/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 0.1137 - accuracy: 0.9674\n",
            "Epoch 41/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 0.1053 - accuracy: 0.9698\n",
            "Epoch 42/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 0.1047 - accuracy: 0.9705\n",
            "Epoch 43/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 0.1049 - accuracy: 0.9698\n",
            "Epoch 44/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 0.0963 - accuracy: 0.9721\n",
            "Epoch 45/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 0.0985 - accuracy: 0.9717\n",
            "Epoch 46/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 0.0908 - accuracy: 0.9735\n",
            "Epoch 47/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 0.0952 - accuracy: 0.9724\n",
            "Epoch 48/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 0.0874 - accuracy: 0.9750\n",
            "Epoch 49/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 0.0893 - accuracy: 0.9739\n",
            "Epoch 50/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 0.0866 - accuracy: 0.9749\n",
            "Epoch 51/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 0.0835 - accuracy: 0.9752\n",
            "Epoch 52/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 0.0830 - accuracy: 0.9762\n",
            "Epoch 53/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 0.0825 - accuracy: 0.9755\n",
            "Epoch 54/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 0.0779 - accuracy: 0.9772\n",
            "Epoch 55/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 0.0782 - accuracy: 0.9771\n",
            "Epoch 56/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 0.0733 - accuracy: 0.9786\n",
            "Epoch 57/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 0.0761 - accuracy: 0.9775\n",
            "Epoch 58/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 0.0764 - accuracy: 0.9771\n",
            "Epoch 59/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 0.0725 - accuracy: 0.9785\n",
            "Epoch 60/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 0.0696 - accuracy: 0.9793\n",
            "Epoch 61/100\n",
            "2109/2109 [==============================] - 33s 16ms/step - loss: 0.0741 - accuracy: 0.9777\n",
            "Epoch 62/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 0.0700 - accuracy: 0.9788\n",
            "Epoch 63/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 0.0679 - accuracy: 0.9799\n",
            "Epoch 64/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 0.0673 - accuracy: 0.9796\n",
            "Epoch 65/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 0.0660 - accuracy: 0.9804\n",
            "Epoch 66/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 0.0641 - accuracy: 0.9806\n",
            "Epoch 67/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 0.0680 - accuracy: 0.9800\n",
            "Epoch 68/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 0.0649 - accuracy: 0.9806\n",
            "Epoch 69/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 0.0618 - accuracy: 0.9808\n",
            "Epoch 70/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 0.0614 - accuracy: 0.9816\n",
            "Epoch 71/100\n",
            "2109/2109 [==============================] - 33s 15ms/step - loss: 0.0606 - accuracy: 0.9811\n",
            "Epoch 72/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 0.0593 - accuracy: 0.9819\n",
            "Epoch 73/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 0.0574 - accuracy: 0.9824\n",
            "Epoch 74/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 0.0611 - accuracy: 0.9812\n",
            "Epoch 75/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 0.0588 - accuracy: 0.9819\n",
            "Epoch 76/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 0.0577 - accuracy: 0.9824\n",
            "Epoch 77/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 0.0566 - accuracy: 0.9823\n",
            "Epoch 78/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 0.0548 - accuracy: 0.9830\n",
            "Epoch 79/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 0.0581 - accuracy: 0.9822\n",
            "Epoch 80/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 0.0573 - accuracy: 0.9827\n",
            "Epoch 81/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 0.0527 - accuracy: 0.9841\n",
            "Epoch 82/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 0.0549 - accuracy: 0.9832\n",
            "Epoch 83/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 0.0554 - accuracy: 0.9830\n",
            "Epoch 84/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 0.0518 - accuracy: 0.9838\n",
            "Epoch 85/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 0.0524 - accuracy: 0.9837\n",
            "Epoch 86/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 0.0525 - accuracy: 0.9836\n",
            "Epoch 87/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 0.0528 - accuracy: 0.9834\n",
            "Epoch 88/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 0.0510 - accuracy: 0.9841\n",
            "Epoch 89/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 0.0504 - accuracy: 0.9842\n",
            "Epoch 90/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 0.0521 - accuracy: 0.9841\n",
            "Epoch 91/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 0.0486 - accuracy: 0.9851\n",
            "Epoch 92/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 0.0499 - accuracy: 0.9846\n",
            "Epoch 93/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 0.0491 - accuracy: 0.9849\n",
            "Epoch 94/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 0.0484 - accuracy: 0.9850\n",
            "Epoch 95/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 0.0484 - accuracy: 0.9849\n",
            "Epoch 96/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 0.0497 - accuracy: 0.9843\n",
            "Epoch 97/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 0.0445 - accuracy: 0.9859\n",
            "Epoch 98/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 0.0486 - accuracy: 0.9848\n",
            "Epoch 99/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 0.0453 - accuracy: 0.9853\n",
            "Epoch 100/100\n",
            "2109/2109 [==============================] - 32s 15ms/step - loss: 0.0475 - accuracy: 0.9850\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_6_layer_call_fn, lstm_cell_6_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: ram://ae9ad397-8607-4018-8188-900db0117a47/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: ram://ae9ad397-8607-4018-8188-900db0117a47/assets\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fd07433e510> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (X.shape)\n",
        "prediction = model.predict(X[0].reshape(1,sequence_length))\n",
        "print (prediction.shape)\n",
        "print (prediction)"
      ],
      "metadata": {
        "id": "oQxjvh8ggpM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Trying different phrases:\n",
        "## Hi how are you doing today sir\n",
        "# 'I am a student at' --> \"i am a student at columbia university studying rescue project i\"\n",
        "# \"I am a dual degree\" --> \"I am a dual degree master s student at i am\"\n",
        "# \"my name is\" --> \"my name is shreyans kothari and i am a dual degree masters\"\n",
        "# \"Data Science and Machine Learning\" --> \"Data Science and Machine Learning data collection instruments art from 2 the mindless is\"\n",
        "# \"Artificial Intelligence is going to\" --> \"Artificial Intelligence is going to you today is you a others as you big\"\n",
        "test = ['My name is']\n",
        "\n",
        "\n",
        "for t in test:\n",
        "    example = tokenizer.texts_to_sequences([t])\n",
        "    prediction = model.predict(np.array(example))\n",
        "    predicted_word = np.argmax(prediction)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))  # https://stackoverflow.com/a/43927939/246508\n",
        "    t1 = t + \" \" + reverse_word_map[predicted_word]\n",
        "\n",
        "    word2 = tokenizer.texts_to_sequences([t1])\n",
        "    prediction1 = model.predict(np.array(word2))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t2 = t1 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    word2 = tokenizer.texts_to_sequences([t2])\n",
        "    prediction1 = model.predict(np.array(word2))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t3 = t2 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    word2 = tokenizer.texts_to_sequences([t3])\n",
        "    prediction1 = model.predict(np.array(word2))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t4 = t3 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    word3 = tokenizer.texts_to_sequences([t4])\n",
        "    prediction1 = model.predict(np.array(word3))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t5 = t4 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    word4 = tokenizer.texts_to_sequences([t5])\n",
        "    prediction1 = model.predict(np.array(word4))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t6 = t5 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    word5 = tokenizer.texts_to_sequences([t6])\n",
        "    prediction1 = model.predict(np.array(word5))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t7 = t6 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    word6 = tokenizer.texts_to_sequences([t7])\n",
        "    prediction1 = model.predict(np.array(word6))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t8 = t7 + \" \" + reverse_word_map[predicted_word1]  \n",
        "\n",
        "    word7 = tokenizer.texts_to_sequences([t8])\n",
        "    prediction1 = model.predict(np.array(word7))\n",
        "    predicted_word1 = np.argmax(prediction1)\n",
        "    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
        "    t9 = t8 + \" \" + reverse_word_map[predicted_word1]\n",
        "\n",
        "    print(t9)\n",
        "    #print (\"{0} -> {1}\".format(t, reverse_word_map[predicted_word]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LjVicrLfbSy",
        "outputId": "b5453521-1edc-4395-8a6f-15809c5f60af"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My name is shreyans kothari and i am a dual degree masters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 7 performed the best out of all the models. It achieved an accuracy rate of 0.985- the highest of all. The predicitons from the model support the high accuracy rate; the model learned from the training text really well. When I input \"My name is\" the model outputs \"My name is Shreyans Kothari and I am a dual degree...\"; I have used that same sentence in a lot of cover letters/emails. However, it still isn't able to give outputs that make contextual and grammatical sense when I input random words/phrases like \"Artificial Intelligence is going to\". This just goes to say that the model is fit really well on my data but is not generlizable enough to perform good on other texts as well. "
      ],
      "metadata": {
        "id": "V8ZKwAtpZzrV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5) Best Model:\n",
        "Model 7 is the best model of all the model architectures I employed because it fulfills the purpose for which it was created. I hoped to develop a model that would understand my style and learn from my past writings. The results from model 7 conclude that it does just that. The model is very personalized to me . It outputs phrases that I could use in emails/cover letters and other pieces that I write in the future. When I input \"My name is\", it outputs \"my name is shreyans kothari and i am a dual degree masters..\", words I would definitely use in a sentence that begins with \"My name is\". Attaching this model (or a slighly better version) to an app like my email browser will allow me to infuse efficiency into my work by automating mundane tasks like responding to emails."
      ],
      "metadata": {
        "id": "ZC5x4KdYeMx6"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "U_F5ql-tJJid",
        "H5jWvGsDeKG8",
        "0m13JBDOelWN",
        "-qauJ-v4e56O",
        "6BHqGYMke9vo",
        "UtIoIj4OhFqD",
        "ztkVXqFmxV-3",
        "I-jTLbEVAE6O",
        "iQDxre2COftb",
        "rlJ-gKHBgORW",
        "NsdrmAeUu16w",
        "OgA0-Rh-_b7W"
      ],
      "name": "ML_FinalProject_ShreyansKothari.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}