{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c88f5858",
   "metadata": {},
   "source": [
    "# Final Exam\n",
    "***UNI:*** sk4819 | ***Date:*** December 17, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09120e3",
   "metadata": {},
   "source": [
    "### 1.  From the perspective of a social scientist, which models did we learn this semester that are useful for ruling out alternative explanations through control variables AND that allow us to observe substantively meaningful information from model coefficients?\n",
    "Linear regression models, and all of their cousins, are generally in the form  yi = ß0 + ß1x1 +  ß2x2 + e, where yi represents the numeric response for the i-th sample, ß0 represents the estimated intercept, ß1 and ß2 represent the estimated coefficients for the predictors x1 and x2 respectively, and e represents the random error (which cannot be explained by the model). A model that is able to be expressed in this format is deemed \"linear in the parameters.\"\n",
    "\n",
    "This semester, we learned numerous models that fit the following description: models that are useful for ruling out alternative explanations through control variables AND that allow us to observe substantively meaningful information from their coefficients. These consist  of ordinary linear regression models, partial least squares, and penalized models (ridge, lasso, and elastic net). Each of these models seek to find estimates of the parameters that minimize the sum of squared errors. Models of this form are also highly interpretable.\n",
    "\n",
    "All these models lie at different positions on the bias-variance trade-off spectrum; OLS paramter estimates have minimum bias whereas penalized model parameter estimates have lower variance. Which model we choose to work with depends on our objectives with the model. \n",
    "\n",
    "These models allow researchers to study the effect of an independent variable on the dependent variable, by controlling (keeping constant) all the additional independent variables; this allows resarchers to make causal claims about the relationship between the dependent and independent variable. Additionally, the coefficients allow researchers to obtain 'substantively meaningful information' about the relationship between x and y, such as the strength and direction of the relationship. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ec5de5",
   "metadata": {},
   "source": [
    "### 2. Describe the main differences between supervised and unsupervised learning.\n",
    "One of the main differences between supervised and unsupervised machine learning methods is the presence of the output- the y. \n",
    "\n",
    "Supervised learning is used when we want to predict an outcome from a given input, and we have examples of the input/output pairs. Using the input/output pairs, we develop a machine learning model, which comprise our training data set. We use this model to predict output values on unseen input data. There are two types of supervised ML problems: classification and regression. \n",
    "\n",
    "Unsupervised learning, on the other hand, includes all kinds of machine learning models where there is no known output. There exists no teacher to \"instruct\" the learning algorithms. The learning algorithm is shown input data and asked to extract meaningful information from this data. Unsupervised learning is commonly used for data transformation and dimension reductionality. Transoformation of the data includes creating new, more-interpretable and easy to understand representations of the original data. Dimension reductionality refers to summarizing the essential characteristics of the most important features. One application of dimension reductionality is for visualizations.\n",
    "\n",
    "Another important differentiator between supervised and unsupervised learning models is the ability of the researcher to evaluate the result of the models. In the case of supervised learning, there exist numerous accuracy metrics that allow the researcher to evaluate the model to obtain a quantitative metric which could be used to compare the performance of different models. To evaluate an unsupervised ML model, the researcher would have to manually check the model and the results and compare them to the ground truth. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9be4e2",
   "metadata": {},
   "source": [
    "### 3. Is supervised or unsupervised learning the primary approach that is used by machine learning practitioners?  For whatever approach you think is secondary, why would you use this approach (what's a good reason to use these kinds of models?)\n",
    "The 'primary' approach depends on the field and on the objective behind creation of the model. If the model is created to understand the relationship between two (or more variables) and to use this relationship to predict an outcome, then supervised ML would be the primary approach. But if the objective is to make extremely complex data more easy to understand and interpret by humans, then unsupervised machine learning would be the primary approach.  \n",
    "\n",
    "Commonly in the field of Machine Learning, supervised learning is revered as the primary approach. This is the case because often ML is used as synonym for making predictions, and supervised ML is the most appropriate model for predicting outcomes.\n",
    "\n",
    "Often, even when supervised models are considered primary, unsupervised models are still used in a very meaningful and necessary manner. Unsupervised models could be used in preprocessing to reduce dimensionality in the data, or to create some meaningful and interpretable visualizations in the exploratory analysis phase. Learing new representations of the data can improve the accuracy of supervied algorithms, and could also lead to reduced memory and time consumption. Unsupervised models are very commonly used in natural language processing for topic extractions- to find the unknown topics that the body of text is about. Unsupervised learning is also used in clusterting: partioning data into smaller groups with similar characteristics. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9776ce48",
   "metadata": {},
   "source": [
    "### 4. Which unsupervised learning modeling approaches did we cover this semester?  What are the major differences between these techniques?\n",
    "We covered a lot of unsupervised machine learning models this semester:\n",
    "1. `Principal Component Analysis (PCA)`: We started with studying PCA. We learned that PCA is fundamentally a dimensionality reduction algorithm but can also be used as a visualization tool, for noise filtering, or for feature extraction and engineering. Under PCA, the dataset is rotated in a way such that the rotated features are statistically uncorrelated. On the basis of the importance in explaining the data, a subset of the new feature is selected.\n",
    "2. Next we discussed `cluster analysis`: Clustering algorithms seek to learn an optimal division or discrete labeling of groups of points using the inherent properties in the data.\n",
    "    - `K-means clustering`: The k-means clustering algorithm looks for a pre-determined number of clusters within an unlabeled multidimensional dataset. K-means algorithms are based on two main assumptions:\n",
    "        - The \"cluster center\" is the arithmetic mean of all the points belonging to the cluster.\n",
    "        - Each point is closer to its own cluster center than to other cluster centers.\n",
    "    - `Hierarchichal clustering` (`Agglomerative`): Unlike in k-means clustering, hierarchical clustering does not require a commitment to a class of k. Hierarchical clustering builds a 'hierarchy' in a bottom-up fashion, creating clusters based on distances between each observation. It starts with each point as its own cluster. Then it identifies the closest cluster and merges them. This process is performed iteratively to build out a clustering tree. We have the ability to define a 'cut-point': the point at which the algorithm stops merging clusters.\n",
    "3. `Manifold Learning / Multifold Dimension Scaling`: Very briefly, we also covered manifold learning, an unsupervised learning method that is similar to PCA. Under Multifold Dimension Scaling (MDS), given a distance matrix between points, the algorithm recovers a D-dimensional coordinate representation of the data. Given high-dimensional embedded data, MDS seeks a low-dimensional representation of the data that preserves certain relationships within the data. \n",
    "\n",
    "`PCA vs clustering`: One of the biggest difference between PCA and clustering is that PCA looks for low dimension representation of the observations in the entire dataset that explain a good fraction of the variance in the data, whereas clustering looks for homogeneous subgroups among all the observations. \n",
    "\n",
    "`PCA vs MDS`: In comparision to PCA, MDS is rarely used for anything other than interpretable visualizations of high-dimensional data due to its numerous limitations. MDS has no good framework for handling missing data, whereas PCA has prescribed methods for dealing with missing data. Additionally, the presence of any noise in the data can \"short-circuit\" the algorithm and change the result. Conversely, PCA is better able to handle noise, and is also often used for reducing noise from data. Manifold learning is a nascent field and still requires a lot of research before its potential can be fully harnessed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6871481",
   "metadata": {},
   "source": [
    "### 5.  What are the main benefits of using Principal Components Analysis?\n",
    "Principal Component Analysis, as an unsupervised learning model and a preprocessing tool, has a lot of benefits. Most commonly it is used for dimension reductionality; to capture the most important information from the data and to minimize/remove noise effects. PCA is also commonly used for visualizing high-dimensional datasets. \n",
    "\n",
    "Using PCA for dimensionality reduction involves zeroing out/removing one or more of the smallest principal components from the data. This results in a lower-dimensional projection of the data that preserves most of the data variance from the original data.\n",
    "\n",
    "PCA is also useful for feature extraction- it allows you to find a representation of the data that is better suited to a particular analysis than the raw representation of the original data. Any principal components with variance much larger than the effect of the noise should be relatively unaffected by the noise. Reconstructing the data using just the largest subset of principal components would allow us to preferentially keep the signal and throw out most of the noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcdad5a",
   "metadata": {},
   "source": [
    "### 6. Thinking about neural networks, what are three major differences between a deep multilayer perceptron network and a convolutional neural network model?  Be sure to define any key terms in your explanation.\n",
    "\n",
    "`Neural Network (NN) models` perform multiple stages of processing to come to a decision. Similar to a linear regression model, the prediction/output is expressed as a function of the dependent variables that are weighted by their beta coefficients, or their weights in the case of neural network. In NN, the process of computing weighted sums is repeated multiple times. There are one or more intermediate 'hidden units/layers' in the processing step between the input layer and the output. \n",
    "\n",
    "Matehmatically, just computing a series of weighted sums is the same as computing just one weighted sum (like in the case of linear regression). Thus, in Neural Networks, after computing the weighted sum for each hidden unit in each hidden layer, a `nonlinear function` is applied to the result (activation)- often using `rectifying nonlinearity`, also known as `relu`, or `tangens hyperbolicus`, also known as `tanh`. The result of this function is used in the weighted sum that computes the output/prediction. The activation in the output layer depends on the type of data: if the output data is categorical with 2+ categories we use a softmax transformation, if it is a binary classification problem we use a sigmoid transformation, and we don't use any activation in the case of a regression problem.\n",
    "\n",
    "NN models use `backpropagation` when training a model; backpropagation repeatedly adjusts the weights in the neural network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector.\n",
    "\n",
    "A Perceptron is the simplest decision making algorithm. The perceptron takes certain inputs, multiplies them with the corresponding `weights`, and sums it up. After adding a `bias` term, the sum is activated through a non-linear function to get the output. The ML scientist has the ability to manipulate the weights and bias to get the desired output. In `Multilayer Perceptron (MLP)`, the models use a layer of perceptrons. The process initiates from the `Input Layer`: values from the input nodes are fed into the subsequent layer through multiplication of weights, addition of the bias, and activation using a non-linear function. For each subsequent layers, the output of the current layer acts as the input of the next layer. The last layer is called the `Output Layer` and the layers in-between are called `Hidden Layers`. MLP models are often referred to as `Plain Vanilla Neural Network Models` when they only have a single hidden layer. Models with more than one hidden layers are called `Deep Learning Models`.\n",
    "\n",
    "`Convolutional Neural Network (CNN) models` are relatively new, compared to MLP. CNN is very commonly used with visual data and is mostly used in Robotics and Computer Vision. There are three types of layer in a typical CNN model: a `convolutional layer`, a `pooling layer`, and a `fully connected layer`. The models take in a matrix input and runs it through Kernels/filters to understand the pattern in the data. Each cell of the kernel/filter matrix is multiplied to the values in the input matrix to extract the 'convolved' features. In the pooling layer, the size of the matrix is reduced by pooling the values based on a pre-defined function (most commonly, `Max Pooling` is used to get the maximum value in the kernel). This is done to reduce nosie and to extract the most salient features. \n",
    "___\n",
    "\n",
    "Major differences between MLP and CNN modesl:\n",
    "- CNN has less parameters and tries to reduce the dimensions of the data, whereas in MLP models, the number of parameters depend on the dataset\n",
    "- The CNN layers are sparsely connected, compared to the fully connected layers in MLP. In MLP models, every single node in every layer is connected to every node in the subsequent layer. Whereas in CNN, the weights are smaller and shared and thus all nodes don't need to be connected. CNN models use special Convolution and Pooling Layers, whereas MLP is just a network of perceptorn layers\n",
    "- MLP models require orders of magnitude more training data to train a model that predicts similar to a CNN model\n",
    "- In CNN models, a value is calculated for each data point based on a filter /Kernel that uses a convolution operation. This reduces the number of weights that the CNN neural network must learn compared to an MLP model, and this also means that when the location of these features changes it does not throw the neural network off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c844fbb8",
   "metadata": {},
   "source": [
    "### 7. Write the tf.keras code for a multilayer perceptron neural network with the following structure: Three hidden layers.  50 hidden units in the first hidden layer, 100 in the second, and 150 in the third.  Activate all hidden layers with relu.  The output layer should be built to classify to five categories.  Further, your optimization technique should be stochastic gradient descent.  (This code should simply build the architecture of the model and your approach to compile the model.  You will not run it on real data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3931187d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_dim =  ___))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(150, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28ff304",
   "metadata": {},
   "source": [
    "### 8. Write the tf.keras code for a multilayer perceptron neural network with the following structure: Two hidden layers.  75 hidden units in the first hidden layer and 150 in the second.  Activate all hidden layers with relu.  The output layer should be built to classify a binary dependent variable.  Further, your optimization technique should be stochastic gradient descent. (This code should simply build the architecture of the model and your approach to compile the model.  You will not run it on real data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "954abc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(75, activation='relu', input_dim =  __))\n",
    "model.add(Dense(150, activation='relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509f31bf",
   "metadata": {},
   "source": [
    "### 9.  Write the tf.keras code for a convolutional neural network with the following structure: Two convolutional layers.  16 filters in the first layer and 28 in the second.  Activate all convolutional layers with relu.  Use max pooling after each convolutional layer with a 2 by 2 filter.  The output layer should be built to classify to ten categories.  Further, your optimization technique should be stochastic gradient descent.  (This code should simply build the architecture of the model and your approach to compile the model.  You will not run it on real data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbf203b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "\n",
    "nb_conv = convolution kernel size\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, (nb_conv, nb_conv), input_shape=____))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(28, (nb_conv, nb_conv)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  \n",
    "model.add(Flatten())\n",
    "model.add(Dense(28, activation = 'relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5480f2",
   "metadata": {},
   "source": [
    "### 10.  Write the keras code for a convolutional neural network with the following structure: Two convolutional layers.  32 filters in the first layer and 32 in the second.  Activate all convolutional layers with relu.  Use max pooling after each convolutional layer with a 2 by 2 filter.  Add two fully connected layers with 128 hidden units in each layer and relu activations.  The output layer should be built to classify to six categories.  Further, your optimization technique should be stochastic gradient descent.  (This code should simply build the architecture of the model and your approach to compile the model.  You will not run it on real data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8b9544",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_conv = convolution kernel size\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (nb_conv, nb_conv), input_shape = ____))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (nb_conv, nb_conv)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(28, activation = 'relu'))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978c5e68",
   "metadata": {},
   "source": [
    "### Bonus question\n",
    "AI Model Share username: shreyko"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
